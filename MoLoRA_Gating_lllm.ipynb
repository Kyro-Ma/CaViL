{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc8fe31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11050,
     "status": "ok",
     "timestamp": 1761853495126,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "ccc8fe31",
    "outputId": "6968a790-b381-4f1b-e2e3-030626add8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0 | dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, random, gc\n",
    "\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/cache\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"\n",
    "os.environ[\"TMPDIR\"] = \"/root/autodl-tmp/tmp\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/root/autodl-tmp/torch\"\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# PyTorch perf knobs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "print(f\"Device: {device} | dtype: {dtype}\")\n",
    "\n",
    "# Setting up the Hugging Face model endpoint and cache location\n",
    "# These variables must be set in your shell or at the beginning of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d061084-2c89-4ddd-ab33-f2a510546762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749387f3-0faf-45ab-886e-f365bd9c44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate virtual environment (in the terminal or JupyterLab shell)\n",
    "# Ensure that `kyro` environment is activated before running the notebook\n",
    "# !conda activate kyro  # This should be run in a separate Jupyter cell or shell\n",
    "\n",
    "# Change the root data directory for the server environment\n",
    "LAVIC_DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Adapter/category names (ensure the categories match what you want for the model)\n",
    "gate_categories = [\n",
    "    \"amazon_home\",\n",
    "    \"amazon_fashion\",\n",
    "    \"all_beauty\",\n",
    "    \"Appliances\",\n",
    "    \"Arts_Crafts_and_Sewing\",\n",
    "    \"Automotive\",\n",
    "    \"Baby_Products\",\n",
    "    \"Books\",\n",
    "    \"CDs_and_Vinyl\",\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Digital_Music\",\n",
    "    \"Electronics\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\",\n",
    "    \"Pet_Supplies\",\n",
    "    \"Software\",\n",
    "    \"Sports_and_Outdoors\",\n",
    "    \"Toys_and_Games\",\n",
    "    \"Video_Games\",\n",
    "]\n",
    "\n",
    "# Fixed category-to-index mapping to avoid order dependence\n",
    "GATE_MAP_PATH = Path(\"gate_label_mapping.json\")\n",
    "if GATE_MAP_PATH.exists():\n",
    "    with GATE_MAP_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        _map = json.load(f)\n",
    "    gate_categories = [_map[str(i)] for i in sorted(map(int, _map.keys()))]\n",
    "else:\n",
    "    gate_categories = sorted(gate_categories)\n",
    "    with GATE_MAP_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({i: c for i, c in enumerate(gate_categories)}, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e40001-1723-4012-8df4-cac48c4f630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2.0\n",
      "/root/miniconda3/lib/python3.12/site-packages/sentence_transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)\n",
    "print(sentence_transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23ea48e-0750-412e-8939-cef064cd69cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model and data setup\n",
    "ST_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_LENGTH = 256\n",
    "epochs = 10\n",
    "batch_size = 128 if torch.cuda.is_available() else 64\n",
    "lr = 1e-3\n",
    "weight_decay = 0.0\n",
    "grad_clip = 1.0\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "# Cache directory setup\n",
    "CACHE_DIR = Path(os.environ.get(\"HF_HOME\", \"/root/autodl-tmp/cache\"))\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Sentence Transformer setup for model inference\n",
    "st_model = SentenceTransformer(\n",
    "    ST_MODEL_NAME,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "st_model.max_seq_length = MAX_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29911177",
   "metadata": {
    "id": "29911177"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"\\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\\n\\n# Model and data setup\\nST_MODEL_NAME = \"/root/autodl-tmp/cache/hub/models--sentence-transformers--all-minilm-l6-v2/snapshots/2424fdd47412fccc66d91719126b420e9fbd7065\"\\nMAX_LENGTH = 256\\nepochs = 10\\nbatch_size = 128 if torch.cuda.is_available() else 64\\nlr = 1e-3\\nweight_decay = 0.0\\ngrad_clip = 1.0\\nseed = 42\\nrandom.seed(seed); torch.manual_seed(seed)\\n\\n# Cache directory setup\\nCACHE_DIR = Path(\"/root/autodl-tmp/cache/\")\\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\\n\\n# Sentence Transformer setup for model inference\\nst_model = SentenceTransformer(\\n    ST_MODEL_NAME,\\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\\n)\\n\\nst_model.max_seq_length = MAX_LENGTH\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Model and data setup\n",
    "ST_MODEL_NAME = \"/root/autodl-tmp/cache/hub/models--sentence-transformers--all-minilm-l6-v2/snapshots/2424fdd47412fccc66d91719126b420e9fbd7065\"\n",
    "MAX_LENGTH = 256\n",
    "epochs = 10\n",
    "batch_size = 128 if torch.cuda.is_available() else 64\n",
    "lr = 1e-3\n",
    "weight_decay = 0.0\n",
    "grad_clip = 1.0\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "# Cache directory setup\n",
    "CACHE_DIR = Path(\"/root/autodl-tmp/cache/\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sentence Transformer setup for model inference\n",
    "st_model = SentenceTransformer(\n",
    "    ST_MODEL_NAME,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "st_model.max_seq_length = MAX_LENGTH\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd9d688-ad32-4e38-b0da-33a6529d163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local ST model successfully.\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Embedding shape: (1, 384)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded local ST model successfully.\")\n",
    "print(st_model)\n",
    "\n",
    "emb = st_model.encode([\"hello world\"])\n",
    "print(\"Embedding shape:\", emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cc9ef0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1307275,
     "status": "ok",
     "timestamp": 1761855170645,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "c0cc9ef0",
    "outputId": "6ec1206e-0adc-414a-a2fd-aff47deeec02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BUILDING SPLIT: train =====\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Appliances.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Arts_Crafts_and_Sewing.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Automotive.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Baby_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Books.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/CDs_and_Vinyl.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Cell_Phones_and_Accessories.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Digital_Music.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Electronics.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Grocery_and_Gourmet_Food.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Handmade_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Health.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Industrial_and_Scientific.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Kindle_Store.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Movies_and_TV.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Musical_Instruments.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Office_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Patio_Lawn_and_Garden.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Pet_Supplies.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Software.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Sports_and_Outdoors.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Toys_and_Games.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/Video_Games.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/all_beauty.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/amazon_fashion.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/train/amazon_home.pt — skipping.\n",
      "\n",
      "===== BUILDING SPLIT: valid =====\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Appliances.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Arts_Crafts_and_Sewing.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Automotive.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Baby_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Books.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/CDs_and_Vinyl.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Cell_Phones_and_Accessories.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Digital_Music.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Electronics.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Grocery_and_Gourmet_Food.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Handmade_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Health.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Industrial_and_Scientific.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Kindle_Store.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Movies_and_TV.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Musical_Instruments.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Office_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Patio_Lawn_and_Garden.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Pet_Supplies.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Software.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Sports_and_Outdoors.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Toys_and_Games.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/Video_Games.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/all_beauty.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/amazon_fashion.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/valid/amazon_home.pt — skipping.\n",
      "\n",
      "===== BUILDING SPLIT: test =====\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Appliances.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Arts_Crafts_and_Sewing.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Automotive.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Baby_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Books.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/CDs_and_Vinyl.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Cell_Phones_and_Accessories.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Digital_Music.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Electronics.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Grocery_and_Gourmet_Food.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Handmade_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Health.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Industrial_and_Scientific.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Kindle_Store.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Movies_and_TV.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Musical_Instruments.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Office_Products.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Patio_Lawn_and_Garden.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Pet_Supplies.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Software.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Sports_and_Outdoors.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Toys_and_Games.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/Video_Games.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/all_beauty.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/amazon_fashion.pt — skipping.\n",
      "[CACHE] Found: /root/autodl-tmp/cache/test/amazon_home.pt — skipping.\n"
     ]
    }
   ],
   "source": [
    "class LaViCGateRawCategory(Dataset):\n",
    "    \"\"\"Loads ONLY ONE category for a given split.\"\"\"\n",
    "    def __init__(self, data_root: Path, category: str, split: str):\n",
    "        self.samples = []\n",
    "        fp = data_root / category / f\"{split}.jsonl\"\n",
    "\n",
    "        if not fp.exists():\n",
    "            print(f\"[WARN] Missing {fp}. Skipping category {category}.\")\n",
    "            return\n",
    "\n",
    "        with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    text = rec.get(\"context\", \"\").strip()\n",
    "                    if text:\n",
    "                        self.samples.append(text)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_cache_category(split: str, st_model: SentenceTransformer, batch: int = 64):\n",
    "    \"\"\"\n",
    "    Build per-category embedding cache:\n",
    "      cache/train/Automotive.pt\n",
    "      cache/valid/Automotive.pt\n",
    "      ...\n",
    "    Uses incremental batching to avoid memory explosion.\n",
    "    \"\"\"\n",
    "    split_dir = CACHE_DIR / split\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx, cat in enumerate(gate_categories):\n",
    "        cache_path = split_dir / f\"{cat}.pt\"\n",
    "\n",
    "        # Already exists → skip\n",
    "        if cache_path.exists():\n",
    "            print(f\"[CACHE] Found: {cache_path} — skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[EMBED] Building cache for {split}/{cat} ...\")\n",
    "\n",
    "        ds = LaViCGateRawCategory(LAVIC_DATA_DIR, cat, split)\n",
    "        if len(ds) == 0:\n",
    "            print(f\"[WARN] No samples for category {cat} ({split}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        X_parts = []\n",
    "        Y_parts = []\n",
    "\n",
    "        # Use small batches to prevent kernel crash\n",
    "        for i in tqdm(range(0, len(ds), batch), desc=f\"Encoding {cat}\"):\n",
    "            chunk = ds.samples[i:i+batch]\n",
    "\n",
    "            emb = st_model.encode(\n",
    "                chunk,\n",
    "                batch_size=min(32, len(chunk)),   # safer batch size\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                show_progress_bar=False,\n",
    "            )\n",
    "\n",
    "            X_parts.append(emb.cpu())\n",
    "            Y_parts.append(torch.full((emb.size(0),), idx, dtype=torch.long))\n",
    "\n",
    "        # Merge\n",
    "        X = torch.cat(X_parts, dim=0)\n",
    "        y = torch.cat(Y_parts, dim=0)\n",
    "\n",
    "        out = {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"category\": cat,\n",
    "            \"category_index\": idx,\n",
    "        }\n",
    "\n",
    "        torch.save(out, cache_path)\n",
    "        print(f\"[CACHE SAVED] {cache_path} | X={tuple(X.shape)} | y={tuple(y.shape)}\")\n",
    "\n",
    "        # free memory\n",
    "        del X_parts, Y_parts, X, y\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "st_model = SentenceTransformer(ST_MODEL_NAME, device=device)\n",
    "st_model.max_seq_length = MAX_LENGTH\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    print(f\"\\n===== BUILDING SPLIT: {split} =====\")\n",
    "    build_cache_category(split, st_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c44aa7",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1761855170668,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "70c44aa7",
    "outputId": "d9823c52-dfa6-4b62-dd45-a32260391fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Loading 26 category cache files from /root/autodl-tmp/cache/train\n",
      "  - Loaded Appliances.pt | (1832, 384) | category=Appliances\n",
      "  - Loaded Arts_Crafts_and_Sewing.pt | (1013, 384) | category=Arts_Crafts_and_Sewing\n",
      "  - Loaded Automotive.pt | (1209, 384) | category=Automotive\n",
      "  - Loaded Baby_Products.pt | (2668, 384) | category=Baby_Products\n",
      "  - Loaded Books.pt | (3765, 384) | category=Books\n",
      "  - Loaded CDs_and_Vinyl.pt | (3054, 384) | category=CDs_and_Vinyl\n",
      "  - Loaded Cell_Phones_and_Accessories.pt | (2860, 384) | category=Cell_Phones_and_Accessories\n",
      "  - Loaded Digital_Music.pt | (1388, 384) | category=Digital_Music\n",
      "  - Loaded Electronics.pt | (3196, 384) | category=Electronics\n",
      "  - Loaded Grocery_and_Gourmet_Food.pt | (2291, 384) | category=Grocery_and_Gourmet_Food\n",
      "  - Loaded Handmade_Products.pt | (2284, 384) | category=Handmade_Products\n",
      "  - Loaded Health.pt | (1440, 384) | category=Health\n",
      "  - Loaded Industrial_and_Scientific.pt | (3182, 384) | category=Industrial_and_Scientific\n",
      "  - Loaded Kindle_Store.pt | (13703, 384) | category=Kindle_Store\n",
      "  - Loaded Movies_and_TV.pt | (1827, 384) | category=Movies_and_TV\n",
      "  - Loaded Musical_Instruments.pt | (3356, 384) | category=Musical_Instruments\n",
      "  - Loaded Office_Products.pt | (2324, 384) | category=Office_Products\n",
      "  - Loaded Patio_Lawn_and_Garden.pt | (918, 384) | category=Patio_Lawn_and_Garden\n",
      "  - Loaded Pet_Supplies.pt | (4175, 384) | category=Pet_Supplies\n",
      "  - Loaded Software.pt | (1821, 384) | category=Software\n",
      "  - Loaded Sports_and_Outdoors.pt | (2558, 384) | category=Sports_and_Outdoors\n",
      "  - Loaded Toys_and_Games.pt | (1527, 384) | category=Toys_and_Games\n",
      "  - Loaded Video_Games.pt | (15313, 384) | category=Video_Games\n",
      "  - Loaded all_beauty.pt | (6142, 384) | category=all_beauty\n",
      "  - Loaded amazon_fashion.pt | (6416, 384) | category=amazon_fashion\n",
      "  - Loaded amazon_home.pt | (2961, 384) | category=amazon_home\n",
      "[MERGED] X=(93223, 384), y=(93223,)\n",
      "[LOAD] Loading 26 category cache files from /root/autodl-tmp/cache/valid\n",
      "  - Loaded Appliances.pt | (229, 384) | category=Appliances\n",
      "  - Loaded Arts_Crafts_and_Sewing.pt | (126, 384) | category=Arts_Crafts_and_Sewing\n",
      "  - Loaded Automotive.pt | (151, 384) | category=Automotive\n",
      "  - Loaded Baby_Products.pt | (333, 384) | category=Baby_Products\n",
      "  - Loaded Books.pt | (470, 384) | category=Books\n",
      "  - Loaded CDs_and_Vinyl.pt | (381, 384) | category=CDs_and_Vinyl\n",
      "  - Loaded Cell_Phones_and_Accessories.pt | (357, 384) | category=Cell_Phones_and_Accessories\n",
      "  - Loaded Digital_Music.pt | (173, 384) | category=Digital_Music\n",
      "  - Loaded Electronics.pt | (399, 384) | category=Electronics\n",
      "  - Loaded Grocery_and_Gourmet_Food.pt | (286, 384) | category=Grocery_and_Gourmet_Food\n",
      "  - Loaded Handmade_Products.pt | (285, 384) | category=Handmade_Products\n",
      "  - Loaded Health.pt | (180, 384) | category=Health\n",
      "  - Loaded Industrial_and_Scientific.pt | (397, 384) | category=Industrial_and_Scientific\n",
      "  - Loaded Kindle_Store.pt | (1711, 384) | category=Kindle_Store\n",
      "  - Loaded Movies_and_TV.pt | (228, 384) | category=Movies_and_TV\n",
      "  - Loaded Musical_Instruments.pt | (419, 384) | category=Musical_Instruments\n",
      "  - Loaded Office_Products.pt | (290, 384) | category=Office_Products\n",
      "  - Loaded Patio_Lawn_and_Garden.pt | (114, 384) | category=Patio_Lawn_and_Garden\n",
      "  - Loaded Pet_Supplies.pt | (521, 384) | category=Pet_Supplies\n",
      "  - Loaded Software.pt | (227, 384) | category=Software\n",
      "  - Loaded Sports_and_Outdoors.pt | (319, 384) | category=Sports_and_Outdoors\n",
      "  - Loaded Toys_and_Games.pt | (190, 384) | category=Toys_and_Games\n",
      "  - Loaded Video_Games.pt | (1914, 384) | category=Video_Games\n",
      "  - Loaded all_beauty.pt | (758, 384) | category=all_beauty\n",
      "  - Loaded amazon_fashion.pt | (797, 384) | category=amazon_fashion\n",
      "  - Loaded amazon_home.pt | (368, 384) | category=amazon_home\n",
      "[MERGED] X=(11623, 384), y=(11623,)\n",
      "Embed dim: 384 | Train 93223 | Valid 11623\n"
     ]
    }
   ],
   "source": [
    "#@title Router dataset (cached tensors) & loaders\n",
    "\"\"\"\n",
    "class TensorGateDataset(Dataset):\n",
    "    def __init__(self, tensor_path: Path):\n",
    "        blob = torch.load(tensor_path, map_location=\"cpu\")\n",
    "        self.X = blob[\"X\"]\n",
    "        self.y = blob[\"y\"]\n",
    "        self.categories = blob[\"categories\"]\n",
    "\n",
    "    def __len__(self): return self.X.size(0)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "def collate(batch):\n",
    "    X, y = zip(*batch)\n",
    "    X = torch.stack(X).to(device=device, dtype=dtype, non_blocking=True)\n",
    "    y = torch.stack(y).to(device=device, non_blocking=True)\n",
    "    return X, y\n",
    "\n",
    "train_ds = TensorGateDataset(CACHE_DIR / \"train.pt\")\n",
    "val_ds   = TensorGateDataset(CACHE_DIR / \"valid.pt\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate, drop_last=False)\n",
    "# num_workers=2 is # of parallel background processes will load data, if we are using server, it can be larger\n",
    "# pin_memory=True is for PyTorch to allocate batches in page-locked (pinned) host memory, is for gup\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate, drop_last=False)\n",
    "\n",
    "embed_dim = train_ds.X.size(1)\n",
    "print(f\"Embed dim: {embed_dim} | Train {len(train_ds)} | Valid {len(val_ds)}\")\n",
    "\"\"\"\n",
    "\n",
    "class TensorGateDataset(Dataset):\n",
    "    \"\"\"Loads ALL category-level cached tensors in a split directory.\"\"\"\n",
    "    def __init__(self, split_dir: Path):\n",
    "        \"\"\"\n",
    "        split_dir:\n",
    "            /root/autodl-tmp/cache/train\n",
    "            /root/autodl-tmp/cache/valid\n",
    "        \"\"\"\n",
    "        self.X_list = []\n",
    "        self.y_list = []\n",
    "        self.category_index_list = []\n",
    "\n",
    "        pt_files = sorted(split_dir.glob(\"*.pt\"))\n",
    "        if len(pt_files) == 0:\n",
    "            raise RuntimeError(f\"No .pt cached files found in {split_dir}\")\n",
    "\n",
    "        print(f\"[LOAD] Loading {len(pt_files)} category cache files from {split_dir}\")\n",
    "\n",
    "        for pt in pt_files:\n",
    "            blob = torch.load(pt, map_location=\"cpu\")\n",
    "\n",
    "            X = blob[\"X\"]          # [Nc, H]\n",
    "            y = blob[\"y\"]          # [Nc]\n",
    "            cat = blob[\"category\"] # str\n",
    "\n",
    "            print(f\"  - Loaded {pt.name} | {tuple(X.shape)} | category={cat}\")\n",
    "\n",
    "            self.X_list.append(X)\n",
    "            self.y_list.append(y)\n",
    "\n",
    "        # Concatenate all categories\n",
    "        self.X = torch.cat(self.X_list, dim=0)\n",
    "        self.y = torch.cat(self.y_list, dim=0)\n",
    "\n",
    "        print(f\"[MERGED] X={tuple(self.X.shape)}, y={tuple(self.y.shape)}\")\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Collate\n",
    "def collate(batch):\n",
    "    X, y = zip(*batch)\n",
    "    X = torch.stack(X).to(device=device, dtype=dtype, non_blocking=True)\n",
    "    y = torch.stack(y).to(device=device, non_blocking=True)\n",
    "    return X, y\n",
    "\n",
    "train_ds = TensorGateDataset(CACHE_DIR / \"train\")\n",
    "val_ds   = TensorGateDataset(CACHE_DIR / \"valid\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # change if GPU\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,   # change if GPU\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "embed_dim = train_ds.X.size(1)\n",
    "print(f\"Embed dim: {embed_dim} | Train {len(train_ds)} | Valid {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceddd97e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17032,
     "status": "ok",
     "timestamp": 1761855187701,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "ceddd97e",
    "outputId": "26d71772-787e-4173-e61c-f35144a14714"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 729/729 [00:01<00:00, 466.65batch/s, loss=1.3634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=1.9360 | val_loss=1.2105 | val_acc=0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 729/729 [00:01<00:00, 510.19batch/s, loss=0.6909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.9570 | val_loss=0.7788 | val_acc=0.8266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 729/729 [00:01<00:00, 544.07batch/s, loss=0.7514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] train_loss=0.6976 | val_loss=0.6288 | val_acc=0.8435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 729/729 [00:01<00:00, 503.93batch/s, loss=0.5831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] train_loss=0.5913 | val_loss=0.5566 | val_acc=0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 729/729 [00:01<00:00, 512.73batch/s, loss=0.5208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] train_loss=0.5340 | val_loss=0.5141 | val_acc=0.8587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 729/729 [00:01<00:00, 523.42batch/s, loss=0.3683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] train_loss=0.4981 | val_loss=0.4867 | val_acc=0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 729/729 [00:01<00:00, 563.06batch/s, loss=0.4981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] train_loss=0.4734 | val_loss=0.4672 | val_acc=0.8664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 729/729 [00:01<00:00, 533.57batch/s, loss=0.5355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] train_loss=0.4552 | val_loss=0.4529 | val_acc=0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 729/729 [00:01<00:00, 519.04batch/s, loss=0.4218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] train_loss=0.4412 | val_loss=0.4419 | val_acc=0.8705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 729/729 [00:01<00:00, 551.80batch/s, loss=0.6967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss=0.4300 | val_loss=0.4333 | val_acc=0.8728\n",
      "[DONE] Saved router to /root/autodl-tmp/cache/llmSimpleRouter/gate_router.pt\n"
     ]
    }
   ],
   "source": [
    "#@title Simple Router & training\n",
    "class SimpleRouter(nn.Module):\n",
    "    def __init__(self, hidden_size, num_adapters):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_adapters)\n",
    "        )\n",
    "    def forward(self, x):  # x: [B, H]\n",
    "        return self.net(x)\n",
    "\n",
    "router = SimpleRouter(hidden_size=embed_dim, num_adapters=len(gate_categories)).to(device).to(dtype)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(router.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    router.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for X, y in val_loader:\n",
    "        logits = router(X)  # [B, C]\n",
    "        loss = criterion(logits, y)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.numel()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "    return (total_loss / max(total, 1)), (correct / max(total, 1))\n",
    "\n",
    "\n",
    "best_val_acc, best_state = 0.0, None\n",
    "for epoch in range(1, epochs + 1):\n",
    "    router.train()\n",
    "    total_train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", unit=\"batch\")\n",
    "    for X, y in pbar:\n",
    "        logits = router(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(router.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * y.size(0)\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    val_loss, val_acc = evaluate()\n",
    "    train_loss = total_train_loss / len(train_ds)\n",
    "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {k: v.detach().cpu() for k, v in router.state_dict().items()}\n",
    "\n",
    "# Save best state\n",
    "CKPT_DIR = Path(\"/root/autodl-tmp/cache/llmSimpleRouter\"); CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(best_state if best_state is not None else router.state_dict(), CKPT_DIR / \"gate_router.pt\")\n",
    "with (CKPT_DIR / \"gate_label_mapping.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({i: cat for i, cat in enumerate(gate_categories)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[DONE] Saved router to {CKPT_DIR/'gate_router.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7275a8f-a51c-4334-9e20-29cef116c55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_adapters = 26\n",
      "router output size: 26\n",
      "Unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "Count: 26\n",
      "0 Appliances count = 2061\n",
      "1 Arts_Crafts_and_Sewing count = 1139\n",
      "2 Automotive count = 1360\n",
      "3 Baby_Products count = 3001\n",
      "4 Books count = 4235\n",
      "5 CDs_and_Vinyl count = 3435\n",
      "6 Cell_Phones_and_Accessories count = 3217\n",
      "7 Digital_Music count = 1561\n",
      "8 Electronics count = 3595\n",
      "9 Grocery_and_Gourmet_Food count = 2577\n",
      "10 Handmade_Products count = 2569\n",
      "11 Health count = 1620\n",
      "12 Industrial_and_Scientific count = 3579\n",
      "13 Kindle_Store count = 15414\n",
      "14 Movies_and_TV count = 2055\n",
      "15 Musical_Instruments count = 3775\n",
      "16 Office_Products count = 2614\n",
      "17 Patio_Lawn_and_Garden count = 1032\n",
      "18 Pet_Supplies count = 4696\n"
     ]
    }
   ],
   "source": [
    "print(\"num_adapters =\", len(gate_categories))\n",
    "print(\"router output size:\", router.net[0].out_features)\n",
    "import torch\n",
    "ys = train_ds.y.tolist() + val_ds.y.tolist()\n",
    "print(\"Unique labels:\", sorted(set(ys)))\n",
    "print(\"Count:\", len(set(ys)))\n",
    "from collections import Counter\n",
    "\n",
    "cnt = Counter(ys)\n",
    "for idx in range(19):\n",
    "    print(idx, gate_categories[idx], \"count =\", cnt.get(idx, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c01dc0",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1761855187989,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "64c01dc0",
    "outputId": "88dce4e4-06d1-404b-fdc0-b37c1490bb12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 91/91 [00:00<00:00, 2073.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes in eval: ['Appliances', 'Arts_Crafts_and_Sewing', 'Automotive', 'Baby_Products', 'Books', 'CDs_and_Vinyl', 'Cell_Phones_and_Accessories', 'Digital_Music', 'Electronics', 'Grocery_and_Gourmet_Food', 'Handmade_Products', 'Health', 'Industrial_and_Scientific', 'Kindle_Store', 'Movies_and_TV', 'Musical_Instruments', 'Office_Products', 'Patio_Lawn_and_Garden', 'Pet_Supplies', 'Software', 'Sports_and_Outdoors', 'Toys_and_Games', 'Video_Games', 'all_beauty', 'amazon_fashion', 'amazon_home']\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Appliances       0.93      0.97      0.95       229\n",
      "     Arts_Crafts_and_Sewing       0.91      0.78      0.84       126\n",
      "                 Automotive       0.85      0.79      0.82       151\n",
      "              Baby_Products       0.90      0.93      0.92       333\n",
      "                      Books       0.78      0.72      0.75       470\n",
      "              CDs_and_Vinyl       0.93      0.93      0.93       381\n",
      "Cell_Phones_and_Accessories       0.94      0.96      0.95       357\n",
      "              Digital_Music       0.95      0.91      0.93       173\n",
      "                Electronics       0.79      0.86      0.82       399\n",
      "   Grocery_and_Gourmet_Food       0.88      0.93      0.90       286\n",
      "          Handmade_Products       0.85      0.87      0.86       285\n",
      "                     Health       0.77      0.67      0.71       180\n",
      "  Industrial_and_Scientific       0.78      0.77      0.78       397\n",
      "               Kindle_Store       0.89      0.92      0.90      1711\n",
      "              Movies_and_TV       0.93      0.92      0.93       228\n",
      "        Musical_Instruments       0.94      0.96      0.95       419\n",
      "            Office_Products       0.83      0.80      0.82       290\n",
      "      Patio_Lawn_and_Garden       0.87      0.83      0.85       114\n",
      "               Pet_Supplies       0.89      0.93      0.91       521\n",
      "                   Software       0.95      0.96      0.96       227\n",
      "        Sports_and_Outdoors       0.94      0.90      0.92       319\n",
      "             Toys_and_Games       0.86      0.74      0.80       190\n",
      "                Video_Games       0.95      0.95      0.95      1914\n",
      "                 all_beauty       0.81      0.82      0.81       758\n",
      "             amazon_fashion       0.81      0.82      0.81       797\n",
      "                amazon_home       0.56      0.48      0.52       368\n",
      "\n",
      "                   accuracy                           0.87     11623\n",
      "                  macro avg       0.86      0.85      0.86     11623\n",
      "               weighted avg       0.87      0.87      0.87     11623\n",
      "\n",
      "[[ 221    1    0    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    6]\n",
      " [   0   98    0    0    0    0    0    0    0    0    2    0    5    2\n",
      "     0    0    5    0    0    0    1    0    0    3    9    1]\n",
      " [   2    0  120    0    0    0    1    0    6    0    0    0    4    0\n",
      "     0    0    2    0    1    0    0    0    0    2    2   11]\n",
      " [   0    0    0  310    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    1    1    4    0    0    0    0    4    5    8]\n",
      " [   0    0    0    0  339    1    0    0    0    0    0    0    0  126\n",
      "     1    0    1    0    0    0    0    1    1    0    0    0]\n",
      " [   0    0    0    0    0  356    0    3    0    1    0    0    5    6\n",
      "     1    1    0    1    0    0    0    0    0    2    5    0]\n",
      " [   0    0    1    0    0    0  341    1    2    0    0    0    1    1\n",
      "     0    0    1    0    0    0    0    0    1    1    6    1]\n",
      " [   0    0    0    0    0    4    1  158    0    0    0    0    0    0\n",
      "     1    7    0    0    0    0    0    0    2    0    0    0]\n",
      " [   1    0    2    0    2    2    6    0  344    0    0    0   21    1\n",
      "     0    4    4    0    0    2    0    1    3    2    1    3]\n",
      " [   0    0    0    1    0    1    0    0    1  266    2    4    0    0\n",
      "     0    0    0    1    0    0    0    0    0    2    0    8]\n",
      " [   0    1    0    4    2    0    0    0    1    1  248    0    0    0\n",
      "     0    1    2    0    4    0    0    3    3    7    6    2]\n",
      " [   1    0    0    0    0    0    1    0    2    5    0  120    1    0\n",
      "     1    0    2    0    0    0    2    0    1   39    2    3]\n",
      " [   1    3    0    0    2    0    0    0   46    0    2    0  306    2\n",
      "     0    1    6    0    3    1    0    0    0    6    3   15]\n",
      " [   0    0    0    0   85    1    0    0    1    4    0    0    1 1574\n",
      "     2    0    0    1    1    0    0    0   33    5    2    1]\n",
      " [   0    0    0    0    1    4    0    0    0    0    0    0    0    7\n",
      "   210    0    1    0    0    0    0    0    3    0    2    0]\n",
      " [   0    0    1    0    2    4    0    2    2    0    1    0    1    1\n",
      "     0  402    0    0    0    1    1    0    1    0    0    0]\n",
      " [   1    0    0    0    3    2    2    0    2    0    0    0   17    3\n",
      "     0    3  232    0    0    0    1    0    6    3    7    8]\n",
      " [   0    0    0    0    0    0    0    0    0    2    1    2    2    0\n",
      "     0    0    0   95    4    0    0    0    1    3    0    4]\n",
      " [   1    1    1    1    0    0    0    0    1    2    0    2    3    2\n",
      "     0    0    1    3  487    0    0    0    0    4    4    8]\n",
      " [   0    0    0    0    0    0    2    0    1    0    0    0    0    0\n",
      "     0    0    0    0    0  219    0    0    3    0    2    0]\n",
      " [   0    0    0    0    0    1    0    0    0    0    2    0    1    3\n",
      "     0    1    0    0    0    0  287    1    0    1   14    8]\n",
      " [   1    0    2    0    0    1    0    0    1    0    0    0    0    1\n",
      "     6    0    0    0    0    0    0  140   26    0    5    7]\n",
      " [   0    0    0    1    1    1    0    3    7    0    1    0    0   39\n",
      "     0    0    5    1    0    5    0    9 1827    1   10    3]\n",
      " [   2    0    6    2    0    1    5    0    2    4    5   23    2    4\n",
      "     1    0    1    1    9    0    3    2    4  619   39   23]\n",
      " [   0    1    2    9    0    1    4    0    5    2   24    1    5    3\n",
      "     1    3    3    0    6    1    8    0    6   45  650   17]\n",
      " [   7    3    6   15    0    2    1    0   12   15    5    4   16    1\n",
      "     1    4   11    5   28    1    3    5    2   18   27  176]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title 5. Evaluation report (fixed & minimal modification)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ==== 修复 1：加载模型时确保 dtype + device 一致 ====\n",
    "router2 = SimpleRouter(hidden_size=embed_dim, num_adapters=len(gate_categories)).to(device).to(dtype)\n",
    "state = torch.load(CKPT_DIR / \"gate_router.pt\", map_location=device)\n",
    "router2.load_state_dict(state)\n",
    "router2.eval()\n",
    "\n",
    "# ==== 修复 2：使用训练时保存的 label mapping，确保类别顺序一致 ====\n",
    "with open(CKPT_DIR / \"gate_label_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "# label_map 是 {index: category_name}\n",
    "sorted_indices = sorted(int(i) for i in label_map.keys())\n",
    "sorted_names   = [label_map[str(i)] for i in sorted_indices]\n",
    "\n",
    "# ==== 运行验证 ====\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        logits = router2(X)\n",
    "        preds = logits.argmax(dim=-1).cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "# validation 中实际出现的类别\n",
    "present_classes = sorted(set(all_labels))\n",
    "present_names   = [label_map[str(i)] for i in present_classes]\n",
    "\n",
    "print(\"Detected classes in eval:\", present_names)\n",
    "\n",
    "# ==== 修复 3：report 使用 present_classes, 避免缺失类报错 ====\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=present_classes,\n",
    "    target_names=present_names,\n",
    "    zero_division=0   # 防止出现未预测类别时报 warning\n",
    "))\n",
    "\n",
    "# 输出 confusion matrix（按出现过的类）\n",
    "print(confusion_matrix(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=present_classes\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "SqVBGPFUPgAX",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1761855188045,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "SqVBGPFUPgAX",
    "outputId": "f58ffd56-9fa8-44e9-af09-35feefaedd0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts per class: {'Appliances': 1832, 'Arts_Crafts_and_Sewing': 1013, 'Automotive': 1209, 'Baby_Products': 2668, 'Books': 3765, 'CDs_and_Vinyl': 3054, 'Cell_Phones_and_Accessories': 2860, 'Digital_Music': 1388, 'Electronics': 3196, 'Grocery_and_Gourmet_Food': 2291, 'Handmade_Products': 2284, 'Health': 1440, 'Industrial_and_Scientific': 3182, 'Kindle_Store': 13703, 'Movies_and_TV': 1827, 'Musical_Instruments': 3356, 'Office_Products': 2324, 'Patio_Lawn_and_Garden': 918, 'Pet_Supplies': 4175, 'Software': 1821, 'Sports_and_Outdoors': 2558, 'Toys_and_Games': 1527, 'Video_Games': 15313, 'all_beauty': 6142, 'amazon_fashion': 6416, 'amazon_home': 2961}\n",
      "[DEBUG] Boost amazon_home weight -> 1.5155\n",
      "Class weights: {'Appliances': 1.2247132062911987, 'Arts_Crafts_and_Sewing': 2.214881181716919, 'Automotive': 1.855810284614563, 'Baby_Products': 0.840957522392273, 'Books': 0.5959295034408569, 'CDs_and_Vinyl': 0.7346675395965576, 'Cell_Phones_and_Accessories': 0.7845016121864319, 'Digital_Music': 1.6164802312850952, 'Electronics': 0.702025830745697, 'Grocery_and_Gourmet_Food': 0.9793428778648376, 'Handmade_Products': 0.982344388961792, 'Health': 1.5581073760986328, 'Industrial_and_Scientific': 0.7051146030426025, 'Kindle_Store': 0.1637360155582428, 'Movies_and_TV': 1.2280648946762085, 'Musical_Instruments': 0.6685562133789062, 'Office_Products': 0.965436577796936, 'Patio_Lawn_and_Garden': 2.444089889526367, 'Pet_Supplies': 0.5374071002006531, 'Software': 1.2321113348007202, 'Sports_and_Outdoors': 0.8771206736564636, 'Toys_and_Games': 1.4693350791931152, 'Video_Games': 0.14652089774608612, 'all_beauty': 0.36530032753944397, 'amazon_fashion': 0.34969988465309143, 'amazon_home': 1.51548433303833}\n"
     ]
    }
   ],
   "source": [
    "#@title Inspect split distribution & make class weights\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "counts = torch.bincount(train_ds.y)\n",
    "class_names = gate_categories\n",
    "print(\"Train counts per class:\", dict(zip(class_names, counts.tolist())))\n",
    "\n",
    "# inverse-frequency weights (normalized)\n",
    "class_weights = (counts.sum() / (counts + 1e-8)).float()\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "# extra boost for underperforming class (if present)\n",
    "if \"amazon_home\" in class_names:\n",
    "    idx = class_names.index(\"amazon_home\")\n",
    "    class_weights[idx] *= 2.0\n",
    "    print(f\"[DEBUG] Boost amazon_home weight -> {class_weights[idx].item():.4f}\")\n",
    "\n",
    "print(\"Class weights:\", dict(zip(class_names, class_weights.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "_Yfi-A2JPlr0",
   "metadata": {
    "cellView": "form",
    "id": "_Yfi-A2JPlr0"
   },
   "outputs": [],
   "source": [
    "#@title Balanced sampler DataLoader + Weighted CE + label smoothing criterion\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "\n",
    "sample_weights = class_weights[train_ds.y]\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader_balanced = DataLoader(\n",
    "    train_ds, batch_size=batch_size, sampler=sampler,\n",
    "    num_workers=0, pin_memory=False, collate_fn=collate, drop_last=False\n",
    ")\n",
    "\n",
    "crit_weighted = torch.nn.CrossEntropyLoss(\n",
    "    weight=class_weights.to(device),\n",
    "    label_smoothing=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "xXsEYJfbQEc1",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1761855367841,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "xXsEYJfbQEc1",
    "outputId": "536a1f24-dcdb-41f2-8331-52054f1bd9d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RouterMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#@title RouterMLP (regularized)\n",
    "import torch.nn as nn\n",
    "\n",
    "class RouterMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, num_adapters, width=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, width),\n",
    "            nn.LayerNorm(width),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(width, num_adapters)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "router_mlp = RouterMLP(embed_dim, len(gate_categories)).to(device).to(dtype)\n",
    "print(router_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mAp5G3u-QKqg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30363,
     "status": "ok",
     "timestamp": 1761855401168,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "mAp5G3u-QKqg",
    "outputId": "3628f169-a230-46e2-974a-c27e295b79ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 729/729 [00:02<00:00, 320.10batch/s, loss=0.5582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.6616 | val_loss=1.4201 | val_acc=0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 729/729 [00:02<00:00, 329.45batch/s, loss=0.5212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.5299 | val_loss=1.3984 | val_acc=0.8236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 729/729 [00:02<00:00, 356.85batch/s, loss=0.4218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] train_loss=0.4939 | val_loss=1.3250 | val_acc=0.8543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 729/729 [00:02<00:00, 347.63batch/s, loss=0.3568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] train_loss=0.4698 | val_loss=1.3298 | val_acc=0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 729/729 [00:02<00:00, 349.75batch/s, loss=0.4489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] train_loss=0.4527 | val_loss=1.3175 | val_acc=0.8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 729/729 [00:02<00:00, 345.03batch/s, loss=0.4983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] train_loss=0.4425 | val_loss=1.3340 | val_acc=0.8480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 729/729 [00:02<00:00, 358.97batch/s, loss=0.4631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] train_loss=0.4297 | val_loss=1.3044 | val_acc=0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 729/729 [00:02<00:00, 348.58batch/s, loss=0.3729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] train_loss=0.4213 | val_loss=1.2867 | val_acc=0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 729/729 [00:02<00:00, 351.89batch/s, loss=0.4749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] train_loss=0.4157 | val_loss=1.2859 | val_acc=0.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 729/729 [00:02<00:00, 346.60batch/s, loss=0.4295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss=0.4081 | val_loss=1.2885 | val_acc=0.8663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 729/729 [00:02<00:00, 350.50batch/s, loss=0.4718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] train_loss=0.4036 | val_loss=1.2823 | val_acc=0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 729/729 [00:02<00:00, 329.57batch/s, loss=0.3660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] train_loss=0.3986 | val_loss=1.2703 | val_acc=0.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 729/729 [00:02<00:00, 341.68batch/s, loss=0.3743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] train_loss=0.3964 | val_loss=1.2695 | val_acc=0.8769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 729/729 [00:02<00:00, 322.17batch/s, loss=0.3555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] train_loss=0.3931 | val_loss=1.2622 | val_acc=0.8814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 729/729 [00:02<00:00, 355.98batch/s, loss=0.4085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] train_loss=0.3914 | val_loss=1.2629 | val_acc=0.8805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 729/729 [00:02<00:00, 337.71batch/s, loss=0.3911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] train_loss=0.3871 | val_loss=1.2587 | val_acc=0.8815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 729/729 [00:01<00:00, 366.70batch/s, loss=0.4193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] train_loss=0.3862 | val_loss=1.2558 | val_acc=0.8832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 729/729 [00:02<00:00, 336.28batch/s, loss=0.3465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] train_loss=0.3852 | val_loss=1.2589 | val_acc=0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 729/729 [00:02<00:00, 347.07batch/s, loss=0.4368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] train_loss=0.3839 | val_loss=1.2534 | val_acc=0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 729/729 [00:02<00:00, 344.15batch/s, loss=0.3690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] train_loss=0.3836 | val_loss=1.2560 | val_acc=0.8826\n",
      "[DONE] Saved MLP router → /root/autodl-tmp/cache/llmRouterMLP/gate_router_mlp.pt | best_acc=0.8833\n",
      "[DONE] Saved label map → /root/autodl-tmp/cache/llmRouterMLP/gate_label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "#@title Train RouterMLP with balanced loader + cosine schedule + early stop\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_epochs = 20\n",
    "use_balanced = True   # set False to try original loader\n",
    "loader = train_loader_balanced if use_balanced else train_loader\n",
    "\n",
    "criterion = crit_weighted  # or: nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(router_mlp.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-4)\n",
    "patience, bad = 4, 0\n",
    "best_acc, best_state = 0.0, None\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_router(model):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for X, y in val_loader:\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        preds = logits.argmax(-1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.numel()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum / max(total,1), correct / max(total,1)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    router_mlp.train()\n",
    "    run_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{max_epochs}\", unit=\"batch\")\n",
    "    for X, y in pbar:\n",
    "        logits = router_mlp(X)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(router_mlp.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        run_loss += loss.item() * y.size(0)\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    val_loss, val_acc = eval_router(router_mlp)\n",
    "    train_loss = run_loss / len(train_ds)\n",
    "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc, bad = val_acc, 0\n",
    "        best_state = {k: v.detach().cpu() for k, v in router_mlp.state_dict().items()}\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(f\"[EarlyStop] no val acc improvement for {patience} epochs.\")\n",
    "            break\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the stronger router\n",
    "CKPT_DIR2 = Path(\"/root/autodl-tmp/cache/llmRouterMLP\"); CKPT_DIR2.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(best_state if best_state else router_mlp.state_dict(), CKPT_DIR2 / \"gate_router_mlp.pt\")\n",
    "label_map = {str(i): name for i, name in enumerate(gate_categories)}\n",
    "with (CKPT_DIR2 / \"gate_label_mapping.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[DONE] Saved MLP router → {CKPT_DIR2/'gate_router_mlp.pt'} | best_acc={best_acc:.4f}\")\n",
    "print(f\"[DONE] Saved label map → {CKPT_DIR2/'gate_label_mapping.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "YCpd-_JGQj6O",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1761855411136,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "YCpd-_JGQj6O",
    "outputId": "c8349384-bf76-4851-9f4a-5f048817a069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.8833\n",
      "Top-2 accuracy: 0.9509\n",
      "Top-3 accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "#@title Evaluate Top-K accuracy on cached validation set\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_topk(model, val_ds, Ks=(1,2,3)):\n",
    "    X = val_ds.X.to(device=device, dtype=dtype)\n",
    "    y = val_ds.y.to(device)\n",
    "    logits = model(X)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    for K in Ks:\n",
    "        topk = probs.topk(K, dim=-1).indices\n",
    "        acc = (topk == y.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        print(f\"Top-{K} accuracy: {acc:.4f}\")\n",
    "\n",
    "# Load the best MLP router and evaluate\n",
    "router_mlp2 = RouterMLP(embed_dim, len(gate_categories)).to(device).to(dtype)\n",
    "router_mlp2.load_state_dict(torch.load(CKPT_DIR2 / \"gate_router_mlp.pt\", map_location=device))\n",
    "router_mlp2.eval()\n",
    "eval_topk(router_mlp2, val_ds, Ks=(1,2,3))\n",
    "\n",
    "# this will have 1 for top-3 accuracy if we are using 3 cates, because this is the chance of true cate is in top-k\n",
    "# But maybe we do not need the accuracy to be very high in gating, because we will use top-k in MoLoRAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nzzMKAHCQmCS",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1761855421684,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "nzzMKAHCQmCS",
    "outputId": "b27babed-bc54-4748-d70a-60d2c08b1d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] [1.0] top1 0.969990074634552\n"
     ]
    }
   ],
   "source": [
    "#@title Thresholded gate → Top-K fallback at inference\n",
    "# Instead of always choosing a single adapter (Top-1) or always mixing multiple (Top-K), dapts dynamically based on the router’s confidence in its prediction.\n",
    "@torch.no_grad()\n",
    "def gate_weights_thresholded(prompt: str, top_k_fallback: int = 2, tau: float = 0.60):\n",
    "    emb = st_model.encode([prompt], convert_to_tensor=True, device=device, show_progress_bar=False).to(dtype)\n",
    "    logits = router_mlp2(emb)\n",
    "    probs = torch.softmax(logits, dim=-1)  # [1, C]\n",
    "    pmax, imax = probs.max(dim=-1)         # [1]\n",
    "    if pmax.item() >= tau:\n",
    "        idxs = [imax.item()]\n",
    "        ws = [1.0]\n",
    "        mode = \"top1\"\n",
    "    else:\n",
    "        k = min(top_k_fallback, probs.size(-1))\n",
    "        topk = probs.topk(k=k, dim=-1)\n",
    "        idxs = topk.indices.squeeze(0).tolist()\n",
    "        ws = (topk.values.squeeze(0) / topk.values.sum()).tolist()\n",
    "        mode = f\"top{k}\"\n",
    "    return idxs, ws, mode, pmax.item()\n",
    "\n",
    "# Example:\n",
    "idxs, ws, mode, conf = gate_weights_thresholded(\"I need a kettle and toaster set\")\n",
    "print(idxs, ws, mode, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "KvYlfQiuQ4h5",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6558,
     "status": "ok",
     "timestamp": 1761855429326,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "KvYlfQiuQ4h5",
    "outputId": "faac9635-5846-4581-d8e5-d6ee058b5a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated temperature T* = 0.700\n"
     ]
    }
   ],
   "source": [
    "#@title Temperature scaling on validation (grid search)\n",
    "# make the router’s predicted probabilities match reality\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def tune_temperature(model, val_loader, t_min=0.5, t_max=3.0, steps=26):\n",
    "    best_T, best_nll = 1.0, float(\"inf\")\n",
    "    for T in torch.linspace(t_min, t_max, steps=steps, device=device):\n",
    "        nll, n = 0.0, 0\n",
    "        for X, y in val_loader:\n",
    "            logits = model(X) / T\n",
    "            nll += F.cross_entropy(logits, y, reduction=\"sum\").item()\n",
    "            n += y.size(0)\n",
    "        if nll < best_nll:\n",
    "            best_nll, best_T = nll, T.item()\n",
    "    return best_T\n",
    "\n",
    "T_star = tune_temperature(router_mlp2, val_loader)\n",
    "print(f\"Calibrated temperature T* = {T_star:.3f}\")\n",
    "# to use, change torch.softmax(router(emb), dim=-1) to torch.softmax(router(emb) / T_star, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "QgB-fjtnTNyb",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6658,
     "status": "ok",
     "timestamp": 1761855498060,
     "user": {
      "displayName": "Lucia Y",
      "userId": "05128223304788512923"
     },
     "user_tz": -660
    },
    "id": "QgB-fjtnTNyb",
    "outputId": "c965c1a9-ad07-437d-de9f-5eab505be08a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-1: 100%|██████████| 91/91 [00:00<00:00, 531.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Top-1 classification report ====\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Appliances       0.93      0.99      0.96       229\n",
      "     Arts_Crafts_and_Sewing       0.74      0.95      0.83       126\n",
      "                 Automotive       0.81      0.87      0.84       151\n",
      "              Baby_Products       0.92      0.98      0.95       333\n",
      "                      Books       0.65      0.96      0.77       470\n",
      "              CDs_and_Vinyl       0.92      0.94      0.93       381\n",
      "Cell_Phones_and_Accessories       0.96      0.97      0.97       357\n",
      "              Digital_Music       0.87      0.95      0.91       173\n",
      "                Electronics       0.82      0.91      0.86       399\n",
      "   Grocery_and_Gourmet_Food       0.88      0.95      0.91       286\n",
      "          Handmade_Products       0.85      0.94      0.89       285\n",
      "                     Health       0.71      0.80      0.75       180\n",
      "  Industrial_and_Scientific       0.87      0.82      0.84       397\n",
      "               Kindle_Store       0.98      0.83      0.90      1711\n",
      "              Movies_and_TV       0.87      0.96      0.92       228\n",
      "        Musical_Instruments       0.95      0.98      0.96       419\n",
      "            Office_Products       0.89      0.89      0.89       290\n",
      "      Patio_Lawn_and_Garden       0.79      0.94      0.86       114\n",
      "               Pet_Supplies       0.93      0.94      0.94       521\n",
      "                   Software       0.91      1.00      0.95       227\n",
      "        Sports_and_Outdoors       0.91      0.97      0.94       319\n",
      "             Toys_and_Games       0.68      0.92      0.78       190\n",
      "                Video_Games       0.99      0.91      0.95      1914\n",
      "                 all_beauty       0.87      0.79      0.83       758\n",
      "             amazon_fashion       0.89      0.79      0.84       797\n",
      "                amazon_home       0.58      0.52      0.55       368\n",
      "\n",
      "                   accuracy                           0.88     11623\n",
      "                  macro avg       0.85      0.90      0.87     11623\n",
      "               weighted avg       0.89      0.88      0.88     11623\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 226    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    1    2]\n",
      " [   0  120    0    0    0    1    0    0    0    0    1    0    2    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    2]\n",
      " [   0    0  131    0    0    0    0    0    1    0    1    0    1    0\n",
      "     0    2    1    0    0    0    1    0    0    0    1   12]\n",
      " [   0    0    0  327    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    1    0    0    0    0    2    0    2]\n",
      " [   0    0    0    0  452    1    0    0    0    0    1    0    0   11\n",
      "     2    0    0    0    0    0    0    3    0    0    0    0]\n",
      " [   0    0    0    0    0  360    0    6    0    1    0    0    2    1\n",
      "     4    0    2    2    1    0    0    0    0    1    0    1]\n",
      " [   0    0    0    0    0    0  348    0    2    0    0    0    0    0\n",
      "     1    0    1    0    0    0    2    0    0    1    2    0]\n",
      " [   0    0    1    0    0    2    1  165    0    0    0    0    0    0\n",
      "     1    3    0    0    0    0    0    0    0    0    0    0]\n",
      " [   1    0    0    0    0    1    2    1  363    1    0    0   14    0\n",
      "     0    3    1    0    0    0    0    4    0    1    1    6]\n",
      " [   0    0    0    0    0    2    0    0    1  271    0    3    0    0\n",
      "     1    0    0    1    0    0    0    0    0    0    1    6]\n",
      " [   0    1    0    1    2    0    0    0    0    0  267    0    0    0\n",
      "     1    0    1    0    0    0    0    3    0    2    2    5]\n",
      " [   2    0    0    0    0    0    1    0    2    4    0  144    0    0\n",
      "     1    0    0    1    1    0    2    0    0   19    2    1]\n",
      " [   1    6    2    0    1    0    0    0   32    0    2    2  324    0\n",
      "     0    1    3    0    2    1    0    2    0    0    2   16]\n",
      " [   0    2    0    1  231    8    0    4    1    2    0    2    0 1417\n",
      "     8    3    3    3    1    1    1    2   15    2    3    1]\n",
      " [   0    0    0    0    1    4    0    0    0    2    0    0    0    0\n",
      "   220    0    0    0    0    0    0    0    0    0    0    1]\n",
      " [   0    1    2    0    0    1    0    3    0    0    0    0    0    0\n",
      "     0  409    0    0    0    1    0    0    0    0    0    2]\n",
      " [   1    4    0    0    1    3    2    0    3    1    0    0    4    0\n",
      "     1    1  258    1    1    0    2    0    0    1    1    5]\n",
      " [   0    1    0    0    0    0    0    0    0    2    0    1    0    0\n",
      "     0    0    0  107    0    0    0    0    0    1    1    1]\n",
      " [   1    1    1    1    0    0    0    0    1    0    0    0    5    1\n",
      "     0    0    1    5  492    1    0    1    0    1    1    8]\n",
      " [   0    0    0    0    0    0    1    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0  226    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    1    0    0    0    0    2    0    2    0\n",
      "     0    0    0    0    0    0  309    0    0    0    3    2]\n",
      " [   1    1    2    1    0    0    0    0    1    0    0    0    0    0\n",
      "     3    1    0    0    0    0    0  175    1    1    0    3]\n",
      " [   1    0    1    0    9    0    0    9   15    0    8    0    4   20\n",
      "     4    2    4    1    0   16    0   60 1737    1    9   13]\n",
      " [   0    4   11    2    0    2    4    0    3    7    4   42    1    0\n",
      "     3    0    3    1    8    1    6    0    1  596   32   27]\n",
      " [   0    8    0    8    1    3    3    0   10    3   23    3    3    0\n",
      "     1    3    1    4    4    1   13    2    2   46  630   25]\n",
      " [   9   14   11   15    0    1    1    1    9   15    6    6   12    0\n",
      "     1    2   11    9   17    0    2    7    1   11   14  193]]\n",
      "==== Top-K Accuracies ====\n",
      "Top-1: 0.8833\n",
      "Top-2: 0.9509\n",
      "Top-3: 0.9643\n",
      "==== Calibrated temperature T* = 0.700 ====\n",
      "[τ=0.60, K=2] adaptive Top-1/Top-2 accuracy: 0.9216\n",
      "[τ=0.60, K=2] adaptive Top-1/Top-2 accuracy: 0.9046\n",
      "==== Summary ====\n",
      "Top-1 accuracy:   0.8833\n",
      "Top-2 accuracy:   0.9509\n",
      "Top-3 accuracy:   0.9643\n",
      "Thresholded acc (uncalibrated): 0.9216\n",
      "Thresholded acc (calibrated T*): 0.9046\n",
      "Note: Higher Top-K or calibrated thresholded acc → better MoLoRA routing coverage.\n"
     ]
    }
   ],
   "source": [
    "#@title Full Evaluation (Top-K, calibration, thresholded gate)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the trained router\n",
    "router_eval = RouterMLP(embed_dim, len(gate_categories)).to(device).to(dtype)\n",
    "ckpt = torch.load(CKPT_DIR2 / \"gate_router_mlp.pt\", map_location=device)\n",
    "router_eval.load_state_dict(ckpt)\n",
    "router_eval.eval()\n",
    "\n",
    "\n",
    "# Top-1 evaluation\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(val_loader, desc=\"Evaluating Top-1\"):\n",
    "        logits = router_eval(X)\n",
    "        preds = logits.argmax(-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "print(\"==== Top-1 classification report ====\")\n",
    "print(classification_report(all_labels, all_preds, target_names=gate_categories))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Top-K accuracy metrics (K=1,2,3)\n",
    "@torch.no_grad()\n",
    "def eval_topk(model, val_ds, Ks=(1,2,3)):\n",
    "    X = val_ds.X.to(device=device, dtype=dtype)\n",
    "    y = val_ds.y.to(device)\n",
    "    logits = model(X)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    results = {}\n",
    "    for K in Ks:\n",
    "        topk = probs.topk(K, dim=-1).indices\n",
    "        acc = (topk == y.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        results[f\"Top-{K}\"] = acc\n",
    "    return results\n",
    "\n",
    "topk_scores = eval_topk(router_eval, val_ds)\n",
    "print(\"==== Top-K Accuracies ====\")\n",
    "for k, v in topk_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "# Temperature calibration on validation set\n",
    "@torch.no_grad()\n",
    "def tune_temperature(model, val_loader, t_min=0.5, t_max=3.0, steps=26):\n",
    "    best_T, best_nll = 1.0, float(\"inf\")\n",
    "    for T in torch.linspace(t_min, t_max, steps=steps, device=device):\n",
    "        nll, n = 0.0, 0\n",
    "        for X, y in val_loader:\n",
    "            logits = model(X) / T\n",
    "            nll += F.cross_entropy(logits, y, reduction=\"sum\").item()\n",
    "            n += y.size(0)\n",
    "        if nll < best_nll:\n",
    "            best_nll, best_T = nll, T.item()\n",
    "    return best_T\n",
    "\n",
    "T_star = tune_temperature(router_eval, val_loader)\n",
    "print(f\"==== Calibrated temperature T* = {T_star:.3f} ====\")\n",
    "\n",
    "\n",
    "# Thresholded-gate evaluation (Top-1 unless uncertain)\n",
    "@torch.no_grad()\n",
    "def eval_thresholded_gate(model, val_ds, tau=0.6, top_k_fallback=2, T=1.0):\n",
    "    X = val_ds.X.to(device=device, dtype=dtype)\n",
    "    y = val_ds.y.to(device)\n",
    "    logits = model(X) / T\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    pmax, imax = probs.max(dim=-1)\n",
    "    # If confident → Top-1; else → Top-K\n",
    "    topk = probs.topk(top_k_fallback, dim=-1).indices\n",
    "    preds_thresh = []\n",
    "    for i in range(len(y)):\n",
    "        if pmax[i] >= tau:\n",
    "            preds_thresh.append(imax[i].item())\n",
    "        else:\n",
    "            # Choose the top-K category that includes y[i] if any, else top1 fallback\n",
    "            if y[i].item() in topk[i]:\n",
    "                preds_thresh.append(y[i].item())  # counted as correct\n",
    "            else:\n",
    "                preds_thresh.append(imax[i].item())\n",
    "    acc = np.mean(np.array(preds_thresh) == y.cpu().numpy())\n",
    "    print(f\"[τ={tau:.2f}, K={top_k_fallback}] adaptive Top-1/Top-{top_k_fallback} accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "# Run the thresholded evaluation both uncalibrated and calibrated\n",
    "acc_uncal = eval_thresholded_gate(router_eval, val_ds, tau=0.6, top_k_fallback=2, T=1.0)\n",
    "acc_cal   = eval_thresholded_gate(router_eval, val_ds, tau=0.6, top_k_fallback=2, T=T_star)\n",
    "\n",
    "# Summary\n",
    "print(\"==== Summary ====\")\n",
    "print(f\"Top-1 accuracy:   {topk_scores['Top-1']:.4f}\")\n",
    "print(f\"Top-2 accuracy:   {topk_scores['Top-2']:.4f}\")\n",
    "print(f\"Top-3 accuracy:   {topk_scores['Top-3']:.4f}\")\n",
    "print(f\"Thresholded acc (uncalibrated): {acc_uncal:.4f}\")\n",
    "print(f\"Thresholded acc (calibrated T*): {acc_cal:.4f}\")\n",
    "print(\"Note: Higher Top-K or calibrated thresholded acc → better MoLoRA routing coverage.\")\n",
    "\n",
    "# τ is the confidence threshold for the router’s top-1 prediction.If p_max ≥ τ: the model is confident enough → trust Top-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "173046fd-dc6a-4e97-97d4-16b1b6187966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LoRA integration disabled (router-only).\n"
     ]
    }
   ],
   "source": [
    "#@title Integration with LoRA adapters (Top‑1 & Top‑K)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "ENABLE_LORA_INTEGRATION = False  # router-only by default\n",
    "peft_model = None\n",
    "\n",
    "if ENABLE_LORA_INTEGRATION:\n",
    "    BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # placeholder, change to the model LoRA used.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=dtype, device_map=\"auto\")\n",
    "    peft_model = PeftModel(base_model)  # wraps but no adapters yet\n",
    "else:\n",
    "    print(\"[INFO] LoRA integration disabled (router-only).\")\n",
    "\n",
    "# --- Load LoRAs. Each directory should contain adapter_config.json & adapter_model.bin\n",
    "# IMPORTANT: the base model **must match** the base used to train each LoRA.\n",
    "# peft_model.load_adapter(\"/content/lora_ckpts/lm_all_beauty\", adapter_name=\"lm_all_beauty\")\n",
    "# peft_model.load_adapter(\"/content/lora_ckpts/lm_amazon_fashion\", adapter_name=\"lm_amazon_fashion\")\n",
    "# peft_model.load_adapter(\"/content/lora_ckpts/lm_amazon_home\", adapter_name=\"lm_amazon_home\")\n",
    "\n",
    "# Example for loading a single adapter\n",
    "# peft_model = PeftModel.from_pretrained(base_model, \"/content/lora_ckpts/lm_all_beauty\", adapter_name=\"lm_all_beauty\")\n",
    "\n",
    "if peft_model is not None:\n",
    "    peft_model.eval()\n",
    "\n",
    "# --- Reload gate\n",
    "router = SimpleRouter(hidden_size=embed_dim, num_adapters=len(gate_categories)).to(device).to(dtype)\n",
    "router.load_state_dict(torch.load(CKPT_DIR / \"gate_router.pt\", map_location=device))\n",
    "router.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def gate_weights_from_text(prompt: str, top_k: int = 1):\n",
    "    \"\"\"Compute router softmax and return Top‑K (indices, weights).\"\"\"\n",
    "    emb = st_model.encode([prompt], convert_to_tensor=True, device=device, show_progress_bar=False).to(dtype)\n",
    "    logits = router(emb)                  # [1, C]\n",
    "    probs = torch.softmax(logits, dim=-1) # [1, C]\n",
    "    k = min(top_k, probs.size(-1))\n",
    "    topk = torch.topk(probs, k=k, dim=-1)\n",
    "    idxs = topk.indices.squeeze(0).tolist()\n",
    "    ws = topk.values.squeeze(0).tolist()\n",
    "    # normalize to sum=1 for safety\n",
    "    s = sum(ws) + 1e-12\n",
    "    ws = [w/s for w in ws]\n",
    "    return idxs, ws\n",
    "\n",
    "def _ensure_loaded_adapters(adapter_names: List[str]):\n",
    "    if peft_model is None:\n",
    "        raise ValueError(\"peft_model is None. Enable LoRA integration to use adapters.\")\n",
    "    missing = [a for a in adapter_names if a not in peft_model.peft_config]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Adapters not loaded: {missing}. Use peft_model.load_adapter(path, adapter_name) first.\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_top1(prompt: str, adapter_names: List[str], max_new_tokens: int = 64):\n",
    "    idxs, ws = gate_weights_from_text(prompt, top_k=1)\n",
    "    _ensure_loaded_adapters([adapter_names[i] for i in idxs])\n",
    "    chosen = adapter_names[idxs[0]]\n",
    "    peft_model.set_adapter(chosen)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = peft_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True), {\"adapter\": chosen}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_topk(prompt: str, adapter_names: List[str], top_k: int = 2, max_new_tokens: int = 64):\n",
    "    \"\"\"Top‑K mixture inference.\n",
    "    If the PEFT version exposes `add_weighted_adapter`, we can fuse adapters once and run a single generate().\n",
    "    Otherwise we do a safe fallback that mixes logits step‑by‑step (slower).\n",
    "    \"\"\"\n",
    "    idxs, ws = gate_weights_from_text(prompt, top_k=top_k)\n",
    "    names = [adapter_names[i] for i in idxs]\n",
    "    _ensure_loaded_adapters(names)\n",
    "\n",
    "    # Try fast path: weighted fusion into a temporary adapter\n",
    "    if hasattr(peft_model, \"add_weighted_adapter\"):\n",
    "        try:\n",
    "            tmp_name = \"mol_temp\"\n",
    "            # Clean old temp\n",
    "            if tmp_name in getattr(peft_model, \"peft_config\", {}):\n",
    "                peft_model.delete_adapter(tmp_name)\n",
    "            peft_model.add_weighted_adapter(adapters=names, weights=ws, adapter_name=tmp_name)\n",
    "            peft_model.set_adapter(tmp_name)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            out = peft_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "            text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            peft_model.delete_adapter(tmp_name)\n",
    "            return text, {\"adapters\": names, \"weights\": ws, \"mode\": \"weighted_fusion\"}\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Weighted fusion unavailable, falling back to logits mixture:\", e)\n",
    "\n",
    "    # Fallback: per‑step logits mixture (k forward passes per token)\n",
    "    # NOTE: This is slower but works on all PEFT versions.\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attn = inputs.get(\"attention_mask\", None)\n",
    "    generated = input_ids\n",
    "    past_key_values = None\n",
    "    peft_model.generation_config.use_cache = True\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits_sum = None\n",
    "        pkv_next = None\n",
    "        for name, w in zip(names, ws):\n",
    "            peft_model.set_adapter(name)\n",
    "            out = peft_model(input_ids=generated, attention_mask=attn, use_cache=True, past_key_values=past_key_values)\n",
    "            logits_i = out.logits[:, -1, :]  # last token\n",
    "            if logits_sum is None:\n",
    "                logits_sum = w * logits_i\n",
    "                pkv_next = out.past_key_values\n",
    "            else:\n",
    "                logits_sum = logits_sum + w * logits_i\n",
    "\n",
    "        next_id = torch.argmax(logits_sum, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_id], dim=-1)\n",
    "        if attn is not None:\n",
    "            attn = torch.cat([attn, torch.ones_like(next_id)], dim=-1)\n",
    "        past_key_values = pkv_next  # not strictly correct for mixture, keeps speed reasonable\n",
    "\n",
    "    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return text, {\"adapters\": names, \"weights\": ws, \"mode\": \"logits_mixture\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf8c3e7a-aae5-416e-91ff-c926fd1b3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Demo\n",
    "adapter_names = [f\"lm_{c}\" for c in gate_categories]\n",
    "\n",
    "prompt = \"I'm looking for a pair of running shoes for daily jogging—any suggestions?\"\n",
    "# Before running, make sure loaded real adapters whose names match adapter_names.\n",
    "# text1, info1 = generate_top1(prompt, adapter_names, max_new_tokens=64)\n",
    "# print(\"\\n[TOP-1]\", info1, \"\\n\", text1)\n",
    "\n",
    "# textk, infok = generate_topk(prompt, adapter_names, top_k=2, max_new_tokens=64)\n",
    "# print(\"\\n[TOP-K]\", infok, \"\\n\", textk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "926365b4-cb45-4648-bbea-8aa10e89d561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 729/729 [00:03<00:00, 229.60batch/s, loss=0.8074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.9697 | val_loss=2.1110 | val_acc=0.7822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 729/729 [00:03<00:00, 223.60batch/s, loss=0.6451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.7984 | val_loss=2.0490 | val_acc=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 729/729 [00:03<00:00, 215.50batch/s, loss=0.6860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] train_loss=0.7491 | val_loss=2.0121 | val_acc=0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 729/729 [00:03<00:00, 226.91batch/s, loss=0.6907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] train_loss=0.7180 | val_loss=1.9759 | val_acc=0.8429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 729/729 [00:03<00:00, 214.33batch/s, loss=0.7307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] train_loss=0.7057 | val_loss=1.9711 | val_acc=0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 729/729 [00:03<00:00, 204.04batch/s, loss=0.6715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] train_loss=0.6848 | val_loss=1.9534 | val_acc=0.8544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 729/729 [00:03<00:00, 221.40batch/s, loss=0.6671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] train_loss=0.6747 | val_loss=1.9789 | val_acc=0.8414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 729/729 [00:03<00:00, 217.28batch/s, loss=0.6855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] train_loss=0.6648 | val_loss=1.9330 | val_acc=0.8597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 729/729 [00:03<00:00, 221.47batch/s, loss=0.7669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] train_loss=0.6590 | val_loss=1.9360 | val_acc=0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 729/729 [00:03<00:00, 238.06batch/s, loss=0.6926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss=0.6533 | val_loss=1.9398 | val_acc=0.8590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 729/729 [00:03<00:00, 207.55batch/s, loss=0.6780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] train_loss=0.6475 | val_loss=1.9153 | val_acc=0.8690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 729/729 [00:03<00:00, 222.94batch/s, loss=0.6210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] train_loss=0.6415 | val_loss=1.8980 | val_acc=0.8754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 729/729 [00:03<00:00, 229.81batch/s, loss=0.7335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] train_loss=0.6388 | val_loss=1.9116 | val_acc=0.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 729/729 [00:03<00:00, 210.53batch/s, loss=0.6169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] train_loss=0.6344 | val_loss=1.8897 | val_acc=0.8782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 729/729 [00:03<00:00, 210.97batch/s, loss=0.5764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] train_loss=0.6327 | val_loss=1.8949 | val_acc=0.8747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 729/729 [00:03<00:00, 232.36batch/s, loss=0.6407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] train_loss=0.6293 | val_loss=1.8938 | val_acc=0.8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 729/729 [00:03<00:00, 219.33batch/s, loss=0.5910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] train_loss=0.6249 | val_loss=1.9002 | val_acc=0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 729/729 [00:03<00:00, 201.69batch/s, loss=0.6292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] train_loss=0.6240 | val_loss=1.8817 | val_acc=0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 729/729 [00:03<00:00, 210.59batch/s, loss=0.6812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] train_loss=0.6228 | val_loss=1.8755 | val_acc=0.8834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 729/729 [00:03<00:00, 209.23batch/s, loss=0.6047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] train_loss=0.6202 | val_loss=1.8759 | val_acc=0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 729/729 [00:03<00:00, 218.83batch/s, loss=0.6465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] train_loss=0.6202 | val_loss=1.8838 | val_acc=0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 729/729 [00:03<00:00, 212.74batch/s, loss=0.6120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] train_loss=0.6183 | val_loss=1.8851 | val_acc=0.8794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 729/729 [00:03<00:00, 216.13batch/s, loss=0.5859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] train_loss=0.6181 | val_loss=1.8776 | val_acc=0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 729/729 [00:03<00:00, 217.41batch/s, loss=0.6505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] train_loss=0.6170 | val_loss=1.8859 | val_acc=0.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 729/729 [00:03<00:00, 209.51batch/s, loss=0.7001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] train_loss=0.6174 | val_loss=1.8723 | val_acc=0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 729/729 [00:03<00:00, 217.58batch/s, loss=0.6396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] train_loss=0.6150 | val_loss=1.8789 | val_acc=0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 729/729 [00:03<00:00, 219.93batch/s, loss=0.7186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] train_loss=0.6145 | val_loss=1.8756 | val_acc=0.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 729/729 [00:03<00:00, 214.25batch/s, loss=0.7159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] train_loss=0.6153 | val_loss=1.8715 | val_acc=0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 729/729 [00:03<00:00, 220.94batch/s, loss=0.6352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] train_loss=0.6149 | val_loss=1.8731 | val_acc=0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 729/729 [00:03<00:00, 225.77batch/s, loss=0.6244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] train_loss=0.6135 | val_loss=1.8709 | val_acc=0.8857\n",
      "[DONE] Saved MLP+ router → /root/autodl-tmp/cache/llmRouterMLPPlus/gate_router_mlp_plus.pt | best_acc=0.8865\n",
      "[DONE] Saved label map → /root/autodl-tmp/cache/llmRouterMLPPlus/gate_label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "#@title Train RouterMLPPlus (deeper, label smoothing)\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class RouterMLPPlus(nn.Module):\n",
    "    def __init__(self, hidden_size, num_adapters, widths=(512, 256), dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, widths[0]),\n",
    "            nn.LayerNorm(widths[0]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(widths[0], widths[1]),\n",
    "            nn.LayerNorm(widths[1]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(widths[1], num_adapters)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "router_mlp_plus = RouterMLPPlus(embed_dim, len(gate_categories)).to(device).to(dtype)\n",
    "\n",
    "max_epochs = 30\n",
    "use_balanced = True\n",
    "loader = train_loader_balanced if use_balanced else train_loader\n",
    "\n",
    "weight = None\n",
    "if \"class_weights\" in globals():\n",
    "    weight = class_weights.to(device)\n",
    "try:\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight, label_smoothing=0.1)\n",
    "except TypeError:\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "optimizer = AdamW(router_mlp_plus.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-5)\n",
    "patience, bad = 6, 0\n",
    "best_acc, best_state = 0.0, None\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_router_plus(model):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for X, y in val_loader:\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        preds = logits.argmax(-1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.numel()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    router_mlp_plus.train()\n",
    "    run_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{max_epochs}\", unit=\"batch\")\n",
    "    for X, y in pbar:\n",
    "        logits = router_mlp_plus(X)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(router_mlp_plus.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        run_loss += loss.item() * y.size(0)\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    val_loss, val_acc = eval_router_plus(router_mlp_plus)\n",
    "    train_loss = run_loss / len(train_ds)\n",
    "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc, bad = val_acc, 0\n",
    "        best_state = {k: v.detach().cpu() for k, v in router_mlp_plus.state_dict().items()}\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(f\"[EarlyStop] no val acc improvement for {patience} epochs.\")\n",
    "            break\n",
    "    scheduler.step()\n",
    "\n",
    "CKPT_DIR3 = Path(\"/root/autodl-tmp/cache/llmRouterMLPPlus\"); CKPT_DIR3.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(best_state if best_state else router_mlp_plus.state_dict(), CKPT_DIR3 / \"gate_router_mlp_plus.pt\")\n",
    "label_map = {str(i): name for i, name in enumerate(gate_categories)}\n",
    "with (CKPT_DIR3 / \"gate_label_mapping.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[DONE] Saved MLP+ router → {CKPT_DIR3/'gate_router_mlp_plus.pt'} | best_acc={best_acc:.4f}\")\n",
    "print(f\"[DONE] Saved label map → {CKPT_DIR3/'gate_label_mapping.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "facdf66a-cdd2-4b11-8711-a4a79239b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-1: 100%|██████████| 91/91 [00:00<00:00, 534.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Top-1 classification report ====\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Appliances       0.93      0.99      0.96       229\n",
      "     Arts_Crafts_and_Sewing       0.82      0.95      0.88       126\n",
      "                 Automotive       0.80      0.87      0.84       151\n",
      "              Baby_Products       0.93      0.98      0.95       333\n",
      "                      Books       0.64      0.96      0.77       470\n",
      "              CDs_and_Vinyl       0.95      0.95      0.95       381\n",
      "Cell_Phones_and_Accessories       0.96      0.97      0.97       357\n",
      "              Digital_Music       0.91      0.94      0.92       173\n",
      "                Electronics       0.83      0.92      0.87       399\n",
      "   Grocery_and_Gourmet_Food       0.89      0.95      0.92       286\n",
      "          Handmade_Products       0.86      0.92      0.89       285\n",
      "                     Health       0.74      0.83      0.78       180\n",
      "  Industrial_and_Scientific       0.88      0.81      0.84       397\n",
      "               Kindle_Store       0.98      0.83      0.89      1711\n",
      "              Movies_and_TV       0.87      0.94      0.91       228\n",
      "        Musical_Instruments       0.95      0.97      0.96       419\n",
      "            Office_Products       0.87      0.90      0.88       290\n",
      "      Patio_Lawn_and_Garden       0.84      0.89      0.87       114\n",
      "               Pet_Supplies       0.92      0.95      0.94       521\n",
      "                   Software       0.93      0.98      0.95       227\n",
      "        Sports_and_Outdoors       0.94      0.97      0.96       319\n",
      "             Toys_and_Games       0.70      0.90      0.78       190\n",
      "                Video_Games       0.98      0.92      0.95      1914\n",
      "                 all_beauty       0.87      0.79      0.83       758\n",
      "             amazon_fashion       0.90      0.79      0.84       797\n",
      "                amazon_home       0.56      0.61      0.58       368\n",
      "\n",
      "                   accuracy                           0.89     11623\n",
      "                  macro avg       0.86      0.90      0.88     11623\n",
      "               weighted avg       0.90      0.89      0.89     11623\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 226    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    3]\n",
      " [   0  120    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    1    0    0    0    0    0    0    0    0    5]\n",
      " [   0    0  132    0    0    1    0    0    2    0    0    0    1    0\n",
      "     0    1    1    0    2    0    0    0    0    0    0   11]\n",
      " [   0    0    0  325    0    0    0    0    0    0    0    0    1    0\n",
      "     0    0    1    1    1    0    0    0    0    2    0    2]\n",
      " [   0    0    0    0  453    1    0    0    0    1    0    0    0   10\n",
      "     2    0    0    0    0    0    0    3    0    0    0    0]\n",
      " [   0    0    0    0    0  361    0    1    1    2    0    0    5    1\n",
      "     4    1    1    2    1    0    0    0    0    1    0    0]\n",
      " [   0    0    1    0    0    0  348    0    2    0    0    0    1    0\n",
      "     1    0    0    0    0    0    0    0    0    0    3    1]\n",
      " [   0    0    1    0    0    1    0  163    0    0    0    0    0    0\n",
      "     1    5    0    0    0    1    0    0    0    0    1    0]\n",
      " [   0    1    2    0    0    1    1    0  366    1    0    0   11    0\n",
      "     0    3    2    0    0    1    0    2    0    1    2    5]\n",
      " [   0    0    0    0    0    0    0    0    0  273    1    1    0    0\n",
      "     1    0    0    1    0    0    0    0    0    0    1    8]\n",
      " [   0    0    0    3    2    0    0    0    0    0  263    1    1    0\n",
      "     0    0    2    0    0    0    0    5    0    3    2    3]\n",
      " [   1    0    0    0    0    0    1    0    1    4    0  149    0    0\n",
      "     0    0    1    0    1    0    1    0    0   14    1    6]\n",
      " [   1    1    0    0    1    0    1    0   37    0    1    1  322    0\n",
      "     1    1    1    0    4    1    1    2    0    2    2   17]\n",
      " [   0    1    1    2  236    2    0    5    2    2    2    3    0 1413\n",
      "     6    2    2    1    1    1    0    4   17    0    4    4]\n",
      " [   0    0    1    0    2    5    0    0    0    0    0    0    1    2\n",
      "   215    0    0    0    0    0    0    2    0    0    0    0]\n",
      " [   0    0    1    0    1    1    0    3    0    0    0    0    0    0\n",
      "     0  408    0    0    0    0    1    0    1    0    0    3]\n",
      " [   1    2    3    0    1    2    2    0    1    0    0    0    4    0\n",
      "     2    1  260    0    1    0    2    0    0    0    0    8]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    1    1    0\n",
      "     0    0    2  102    2    0    0    0    0    1    0    3]\n",
      " [   0    1    0    0    0    0    0    0    0    1    0    0    7    0\n",
      "     0    0    1    5  497    0    1    0    0    1    2    5]\n",
      " [   0    0    0    0    0    0    2    0    1    0    0    0    0    0\n",
      "     1    0    0    0    0  223    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    1    0    1    0\n",
      "     0    0    0    1    0    0  311    0    0    0    3    2]\n",
      " [   1    1    1    0    0    0    0    0    1    0    0    0    0    0\n",
      "     3    0    0    0    0    0    0  171    4    1    1    6]\n",
      " [   1    0    1    0    8    1    0    7   13    2    8    0    1   22\n",
      "     3    2    6    0    0   10    1   45 1758    4    5   16]\n",
      " [   1    2    6    2    0    2    2    1    2    8    2   39    1    1\n",
      "     3    0    4    1   10    1    4    1    1  596   34   34]\n",
      " [   1    7    5    7    0    2    5    0    7    1   19    2    3    0\n",
      "     2    3    5    1    4    2    9    2    4   49  626   31]\n",
      " [   9   11    9   10    0    2    1    0    6   11    8    3    7    0\n",
      "     1    2    9    6   15    1    0    9    2   11   12  223]]\n",
      "==== Top-K Accuracies ====\n",
      "Top-1: 0.8865\n",
      "Top-2: 0.9513\n",
      "Top-3: 0.9626\n",
      "==== Calibrated temperature T* = 0.700 ====\n",
      "[τ=0.60, K=2] adaptive Top-1/Top-2 accuracy: 0.9272\n",
      "[τ=0.60, K=2] adaptive Top-1/Top-2 accuracy: 0.9069\n",
      "==== Summary ====\n",
      "Top-1 accuracy:   0.8865\n",
      "Top-2 accuracy:   0.9513\n",
      "Top-3 accuracy:   0.9626\n",
      "Thresholded acc (uncalibrated): 0.9272\n",
      "Thresholded acc (calibrated T*): 0.9069\n",
      "Note: Higher Top-K or calibrated thresholded acc → better MoLoRA routing coverage.\n"
     ]
    }
   ],
   "source": [
    "#@title Full Evaluation (Top-K, calibration, thresholded gate) - RouterMLPPlus\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "router_eval_plus = RouterMLPPlus(embed_dim, len(gate_categories)).to(device).to(dtype)\n",
    "ckpt = torch.load(Path(\"/root/autodl-tmp/cache/llmRouterMLPPlus\") / \"gate_router_mlp_plus.pt\", map_location=device)\n",
    "router_eval_plus.load_state_dict(ckpt)\n",
    "router_eval_plus.eval()\n",
    "\n",
    "# Top-1 evaluation\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(val_loader, desc=\"Evaluating Top-1\"):\n",
    "        logits = router_eval_plus(X)\n",
    "        preds = logits.argmax(-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "print(\"==== Top-1 classification report ====\")\n",
    "print(classification_report(all_labels, all_preds, target_names=gate_categories))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Top-K accuracy metrics (K=1,2,3)\n",
    "@torch.no_grad()\n",
    "def eval_topk(model, val_ds, Ks=(1,2,3)):\n",
    "    X = val_ds.X.to(device=device, dtype=dtype)\n",
    "    y = val_ds.y.to(device)\n",
    "    logits = model(X)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    results = {}\n",
    "    for K in Ks:\n",
    "        topk = probs.topk(K, dim=-1).indices\n",
    "        acc = (topk == y.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        results[f\"Top-{K}\"] = acc\n",
    "    return results\n",
    "\n",
    "topk_scores = eval_topk(router_eval_plus, val_ds)\n",
    "print(\"==== Top-K Accuracies ====\")\n",
    "for k, v in topk_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Temperature calibration on validation set\n",
    "@torch.no_grad()\n",
    "def tune_temperature(model, val_loader, t_min=0.5, t_max=3.0, steps=26):\n",
    "    best_T, best_nll = 1.0, float(\"inf\")\n",
    "    for T in torch.linspace(t_min, t_max, steps=steps, device=device):\n",
    "        nll, n = 0.0, 0\n",
    "        for X, y in val_loader:\n",
    "            logits = model(X) / T\n",
    "            nll += F.cross_entropy(logits, y, reduction=\"sum\").item()\n",
    "            n += y.size(0)\n",
    "        if nll < best_nll:\n",
    "            best_nll, best_T = nll, T.item()\n",
    "    return best_T\n",
    "\n",
    "T_star = tune_temperature(router_eval_plus, val_loader)\n",
    "print(f\"==== Calibrated temperature T* = {T_star:.3f} ====\")\n",
    "\n",
    "# Thresholded-gate evaluation (Top-1 unless uncertain)\n",
    "@torch.no_grad()\n",
    "def eval_thresholded_gate(model, val_ds, tau=0.6, top_k_fallback=2, T=1.0):\n",
    "    X = val_ds.X.to(device=device, dtype=dtype)\n",
    "    y = val_ds.y.to(device)\n",
    "    logits = model(X) / T\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    pmax, imax = probs.max(dim=-1)\n",
    "    topk = probs.topk(top_k_fallback, dim=-1).indices\n",
    "    preds_thresh = []\n",
    "    for i in range(len(y)):\n",
    "        if pmax[i] >= tau:\n",
    "            preds_thresh.append(imax[i].item())\n",
    "        else:\n",
    "            if y[i].item() in topk[i]:\n",
    "                preds_thresh.append(y[i].item())\n",
    "            else:\n",
    "                preds_thresh.append(imax[i].item())\n",
    "    acc = np.mean(np.array(preds_thresh) == y.cpu().numpy())\n",
    "    print(f\"[τ={tau:.2f}, K={top_k_fallback}] adaptive Top-1/Top-{top_k_fallback} accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "acc_uncal = eval_thresholded_gate(router_eval_plus, val_ds, tau=0.6, top_k_fallback=2, T=1.0)\n",
    "acc_cal   = eval_thresholded_gate(router_eval_plus, val_ds, tau=0.6, top_k_fallback=2, T=T_star)\n",
    "\n",
    "print(\"==== Summary ====\")\n",
    "print(f\"Top-1 accuracy:   {topk_scores['Top-1']:.4f}\")\n",
    "print(f\"Top-2 accuracy:   {topk_scores['Top-2']:.4f}\")\n",
    "print(f\"Top-3 accuracy:   {topk_scores['Top-3']:.4f}\")\n",
    "print(f\"Thresholded acc (uncalibrated): {acc_uncal:.4f}\")\n",
    "print(f\"Thresholded acc (calibrated T*): {acc_cal:.4f}\")\n",
    "print(\"Note: Higher Top-K or calibrated thresholded acc → better MoLoRA routing coverage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfaa55-288b-4381-8c1b-907aa96a542c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1OsOCKwJ-x43sLD3p5A58VPHKHGRewmBU",
     "timestamp": 1761221219917
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01bbe06980a04df2880cd589f5a5ed55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b1516dd5c0c41e7af43376ae2699686": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f45dbc51fee4896be77963de62dbc65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1643d519e2a5446ba136328e6f5e83af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "208a9ab02887491ea4e5a8e14ffeff7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ec17704b69a4bb09efa425312859b5e",
      "placeholder": "​",
      "style": "IPY_MODEL_9bf03afb363144ceb98a0fcc11f46cad",
      "value": " 551/551 [00:00&lt;00:00, 52.2kB/s]"
     }
    },
    "21a42a0ba3ea4c5e8dcee426e37d9f47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bb8c377b96c401c84507ec7d52d8077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ad48c481c54f12b66323e79eee08f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac7d25e0ba8844c9ac625827cbdbccdf",
      "placeholder": "​",
      "style": "IPY_MODEL_ff7f77b0ffb44d558b8e23986b8d1479",
      "value": " 1.29k/? [00:00&lt;00:00, 56.0kB/s]"
     }
    },
    "32779d37436b41f3ac2afc9444446838": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43dd7ca14e41449bb47d071c05885ebc",
      "placeholder": "​",
      "style": "IPY_MODEL_0b1516dd5c0c41e7af43376ae2699686",
      "value": " 331M/2.20G [00:08&lt;00:40, 46.3MB/s]"
     }
    },
    "3b85b921901a4a39bbf24cc0b0298267": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfb58f19b9a14898af93713aec1602b0",
      "max": 551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a29a18b61b0f41979fcfe04b750f4031",
      "value": 551
     }
    },
    "40a768ee08434d8f9108dd217390f2d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "41295efa25874837856160a6488942a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43dd7ca14e41449bb47d071c05885ebc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c9f51545f2e426c8e1dc4b517bb2c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4723a02b2a44e0198f76ad778c4a6b2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efd67bd01dee4620a37267a0cc62969c",
      "value": 1
     }
    },
    "4dee224ec28d42978b6a9897d72ab482": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4ca2447c78344f787da4f8a2eb5fa24",
      "placeholder": "​",
      "style": "IPY_MODEL_96752cae2fdb4416901206060aa8ea11",
      "value": "tokenizer_config.json: "
     }
    },
    "52161639f3504c5c99f90fc7cb240af3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "656dee3989e44975bdff2f159cd69d02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d8d6f58a6f845e1a8ca2c669d4ef321": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ec17704b69a4bb09efa425312859b5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "726194bbec464a47bb9895d8c0f2506b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7733fdcd7f9842439bbb7acc4b2b2c43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e1c3870691c4deebdbcb0dd2467c933": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e8b9e2fafa44ccb8391dcab28d67810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8763b0a7c224ca39f307d2180e4741f",
      "placeholder": "​",
      "style": "IPY_MODEL_2bb8c377b96c401c84507ec7d52d8077",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "8111250d9b4743e29e36bdb7134b350f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a495b465498a4634b6cbe4efc7b40efa",
       "IPY_MODEL_a2aa9f449d5542cea548aa88fc66cafc",
       "IPY_MODEL_32779d37436b41f3ac2afc9444446838"
      ],
      "layout": "IPY_MODEL_6d8d6f58a6f845e1a8ca2c669d4ef321"
     }
    },
    "88a851759bf041628d53351a1e32f1d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f35217e1c356463e893c0f2bf4bc510d",
       "IPY_MODEL_4c9f51545f2e426c8e1dc4b517bb2c5a",
       "IPY_MODEL_99de05d22d024b5cbfcbbe0e49aef70f"
      ],
      "layout": "IPY_MODEL_7e1c3870691c4deebdbcb0dd2467c933"
     }
    },
    "8a23edfc4a9940198477193c3456fe8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40a768ee08434d8f9108dd217390f2d4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed1ec8552f4946d894bb4e683a698e84",
      "value": 1
     }
    },
    "8a871aa4b5164c67965548da5101d935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdcb55fb580e4839a81189c1382c72d0",
      "placeholder": "​",
      "style": "IPY_MODEL_9bd6e3d4520d4758b3227cc8a57094a1",
      "value": "tokenizer.model: 100%"
     }
    },
    "8e4f042b3e0e4d5ca51c547f12d35268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94d034ff068d497b8ddcd01a9f9f4a31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96752cae2fdb4416901206060aa8ea11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97690f71a77f41fe84b5e3795c5e32a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dee224ec28d42978b6a9897d72ab482",
       "IPY_MODEL_8a23edfc4a9940198477193c3456fe8a",
       "IPY_MODEL_31ad48c481c54f12b66323e79eee08f5"
      ],
      "layout": "IPY_MODEL_e9c831b5ed33493ba3c0bb8376adc38d"
     }
    },
    "99de05d22d024b5cbfcbbe0e49aef70f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01bbe06980a04df2880cd589f5a5ed55",
      "placeholder": "​",
      "style": "IPY_MODEL_9f2d678e4f294dd2af6057d12251f5a4",
      "value": " 1.84M/? [00:00&lt;00:00, 19.4MB/s]"
     }
    },
    "9a69f83eae534d1d8753cfab7c398025": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1643d519e2a5446ba136328e6f5e83af",
      "placeholder": "​",
      "style": "IPY_MODEL_faa9766561bb4fa9bd9cfbefbf50346d",
      "value": " 500k/500k [00:01&lt;00:00, 452kB/s]"
     }
    },
    "9a8b2e580c97430dad79c5be1a920d6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9af1a2d0da7f48d0bc3be1ccdc20719e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e8b9e2fafa44ccb8391dcab28d67810",
       "IPY_MODEL_3b85b921901a4a39bbf24cc0b0298267",
       "IPY_MODEL_208a9ab02887491ea4e5a8e14ffeff7f"
      ],
      "layout": "IPY_MODEL_52161639f3504c5c99f90fc7cb240af3"
     }
    },
    "9bd6e3d4520d4758b3227cc8a57094a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bf03afb363144ceb98a0fcc11f46cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f2d678e4f294dd2af6057d12251f5a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1a95ffb60ca4d49b71337b1f0b39c29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a90af9250aa741dd9881971801bd8e55",
       "IPY_MODEL_aac5b4d025e64483ac2010bb83d294e2",
       "IPY_MODEL_fb0db4acc70f40a49b04c3eb77b9d975"
      ],
      "layout": "IPY_MODEL_94d034ff068d497b8ddcd01a9f9f4a31"
     }
    },
    "a29a18b61b0f41979fcfe04b750f4031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2aa9f449d5542cea548aa88fc66cafc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe644ce9c3d1494c8359e1abf0672165",
      "max": 2200119864,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_726194bbec464a47bb9895d8c0f2506b",
      "value": 331338769
     }
    },
    "a495b465498a4634b6cbe4efc7b40efa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7733fdcd7f9842439bbb7acc4b2b2c43",
      "placeholder": "​",
      "style": "IPY_MODEL_bbecfe6fffb04f51851d6e9bed6e6d0e",
      "value": "model.safetensors:  15%"
     }
    },
    "a7279c91cc9d44d99e31dffb453163c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8432f72f6ff482caeb007c8648def63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a90af9250aa741dd9881971801bd8e55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3921f30732247e6bf405bb5b5bc319b",
      "placeholder": "​",
      "style": "IPY_MODEL_a8432f72f6ff482caeb007c8648def63",
      "value": "config.json: 100%"
     }
    },
    "aac5b4d025e64483ac2010bb83d294e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21a42a0ba3ea4c5e8dcee426e37d9f47",
      "max": 608,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a8b2e580c97430dad79c5be1a920d6f",
      "value": 608
     }
    },
    "ac7d25e0ba8844c9ac625827cbdbccdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5e152fd4db84871ba82d1168d316746": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a871aa4b5164c67965548da5101d935",
       "IPY_MODEL_f1db595dcd5443a2b845da00724b2511",
       "IPY_MODEL_9a69f83eae534d1d8753cfab7c398025"
      ],
      "layout": "IPY_MODEL_41295efa25874837856160a6488942a1"
     }
    },
    "bbecfe6fffb04f51851d6e9bed6e6d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc0de257e0944e3a8d3a1c720c8308fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdcb55fb580e4839a81189c1382c72d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4723a02b2a44e0198f76ad778c4a6b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d4ca2447c78344f787da4f8a2eb5fa24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d856736b204548c49cbb9cbe61ee36b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfb58f19b9a14898af93713aec1602b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9c831b5ed33493ba3c0bb8376adc38d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed1ec8552f4946d894bb4e683a698e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efd67bd01dee4620a37267a0cc62969c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1db595dcd5443a2b845da00724b2511": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc0de257e0944e3a8d3a1c720c8308fb",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f45dbc51fee4896be77963de62dbc65",
      "value": 499723
     }
    },
    "f35217e1c356463e893c0f2bf4bc510d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d856736b204548c49cbb9cbe61ee36b7",
      "placeholder": "​",
      "style": "IPY_MODEL_8e4f042b3e0e4d5ca51c547f12d35268",
      "value": "tokenizer.json: "
     }
    },
    "f3921f30732247e6bf405bb5b5bc319b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8763b0a7c224ca39f307d2180e4741f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faa9766561bb4fa9bd9cfbefbf50346d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb0db4acc70f40a49b04c3eb77b9d975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7279c91cc9d44d99e31dffb453163c7",
      "placeholder": "​",
      "style": "IPY_MODEL_656dee3989e44975bdff2f159cd69d02",
      "value": " 608/608 [00:00&lt;00:00, 32.6kB/s]"
     }
    },
    "fe644ce9c3d1494c8359e1abf0672165": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff7f77b0ffb44d558b8e23986b8d1479": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
