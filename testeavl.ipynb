{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56227432-5f96-4a16-99ed-3c48a6acefc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_MODEL_NAME: llava-hf/llava-v1.6-mistral-7b-hf\n",
      "VISION_ADAPTER_DIR: ./out_distilled/Automotive/vision_lora_adapter_best\n",
      "LM_ADAPTER_DIR: ./out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Automotive.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Automotive\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Automotive/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/cache/\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TMPDIR\"]=\"/root/autodl-tmp/tmp\"\n",
    "os.environ[\"TORCH_HOME\"]=\"/root/autodl-tmp/torch\"\n",
    "os.environ[\"PYTHONDONTWRITEBYTECODE\"]=\"1\"\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaNextForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Config ----\n",
    "CATEGORY = \"Automotive\"\n",
    "CANDIDATE_TYPE = \"candidates_st\"\n",
    "BASE_MODEL_NAME = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "VISION_ADAPTER_DIR = f\"./out_distilled/{CATEGORY}/vision_lora_adapter_best\"\n",
    "LM_ADAPTER_DIR = f\"./out_finetuned/{CATEGORY}/prompt_tuning/trained_lora_adapter\"\n",
    "FINETUNE_OUTPUT_DIR = f\"./test_out/{CATEGORY}\"\n",
    "#DATA_ROOT = Path(\"../data\")\n",
    "DATA_ROOT = Path(\"/root/autodl-tmp/lavic/data\")\n",
    "ITEM_META_PATH = DATA_ROOT / f\"item2meta_train_{CATEGORY}.with_desc.json\"\n",
    "IMAGE_DIR = DATA_ROOT / \"train_images\" / CATEGORY\n",
    "TEST_DATA_PATH = DATA_ROOT / CATEGORY / \"test.jsonl\"\n",
    "BATCH_SIZE = 1\n",
    "MAX_LENGTH = 2048\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda:1\" else torch.float32\n",
    "\n",
    "print(\"BASE_MODEL_NAME:\", BASE_MODEL_NAME)\n",
    "print(\"VISION_ADAPTER_DIR:\", VISION_ADAPTER_DIR)\n",
    "print(\"LM_ADAPTER_DIR:\", LM_ADAPTER_DIR)\n",
    "print(\"ITEM_META_PATH:\", ITEM_META_PATH)\n",
    "print(\"IMAGE_DIR:\", IMAGE_DIR)\n",
    "print(\"TEST_DATA_PATH:\", TEST_DATA_PATH)\n",
    "\n",
    "IMAGE_TOKENS = [\n",
    "    \"<ItemImageEmb1>\", \"<ItemImageEmb2>\", \"<ItemImageEmb3>\",\n",
    "    \"<ItemImageEmb4>\", \"<ItemImageEmb5>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0f6ada-befb-4943-8308-c2fb7e3ba23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /root/autodl-tmp/lavic\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Automotive.with_desc.json\n",
      "resolved: /root/autodl-tmp/lavic/data/item2meta_train_Automotive.with_desc.json\n",
      "exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"cwd:\", Path.cwd())\n",
    "print(\"ITEM_META_PATH:\", ITEM_META_PATH)\n",
    "print(\"resolved:\", ITEM_META_PATH.resolve())\n",
    "print(\"exists:\", ITEM_META_PATH.exists())\n",
    "\n",
    "DATA_ROOT = Path(\"/root/autodl-tmp/lavic/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15722407-210f-4eef-a5b4-384b10e01da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_item_meta(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    rows = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def evaluate_recall_at_k(recommended_ids, gt_items, k=1):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    for rec_id, gts in zip(recommended_ids, gt_items):\n",
    "        for gt in gts:\n",
    "            if rec_id is not None and rec_id == gt:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "    recall = hits / total if total > 0 else 0.0\n",
    "    return recall\n",
    "\n",
    "def check_validity(file_path, model_key):\n",
    "    candidates_key = f\"candidates_{model_key}\"\n",
    "    recommended_key = f\"recommended_{model_key}\"\n",
    "    total = 0\n",
    "    valid = 0\n",
    "    invalid_entries = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            recommended = data.get(recommended_key, None)\n",
    "            candidates = data.get(candidates_key, [])\n",
    "            total += 1\n",
    "            if recommended in candidates:\n",
    "                valid += 1\n",
    "            else:\n",
    "                invalid_entries.append({\n",
    "                    \"line_number\": idx,\n",
    "                    \"recommended_id\": recommended,\n",
    "                    \"candidates\": candidates\n",
    "                })\n",
    "    validity = valid / total if total > 0 else 0.0\n",
    "    return validity, invalid_entries, total\n",
    "\n",
    "def prepare_candidate_info(candidates, item_meta, image_dir, default_image):\n",
    "    candidate_info = []\n",
    "    for cid in candidates:\n",
    "        title = item_meta.get(cid, {}).get('title', 'No Title')\n",
    "        image_path = image_dir / f\"{cid}_0.jpg\"\n",
    "        if image_path.exists():\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        else:\n",
    "            image = default_image\n",
    "        candidate_info.append({\n",
    "            'id': cid,\n",
    "            'title': title,\n",
    "            'image': image\n",
    "        })\n",
    "    return candidate_info\n",
    "\n",
    "CONV_CHARS_PER_TOKEN = 4\n",
    "\n",
    "def build_prompt(conversation_text, candidates_info, max_conv_chars=None):\n",
    "    if max_conv_chars is not None and len(conversation_text) > max_conv_chars:\n",
    "        head_len = max_conv_chars // 2\n",
    "        tail_len = max_conv_chars - head_len\n",
    "        conversation_text = (\n",
    "            conversation_text[:head_len]\n",
    "            + \"\\n...[TRUNCATED]...\\n\"\n",
    "            + conversation_text[-tail_len:]\n",
    "        )\n",
    "    prompt = (\n",
    "        \"You are an AI assistant specialized in providing personalized product recommendations based on user conversations. \"\n",
    "        \"You are given a conversation between a user seeking recommendation (denoted by <submission>) and other users providing comments (denoted by <comment>). \"\n",
    "        \"You are also given a set of candidate products with their IDs, titles and images formatted as \\\"ID: title\\\" followed by an image. \"\n",
    "        \"Among the candidates, recommend the most relevant product to the seeker. \"\n",
    "        \"Only reply with its ID, and don't say anything else.\\n\\n\"\n",
    "        f\"Conversation:\\n{conversation_text}\\n\\n\"\n",
    "        \"Candidates:\\n\"\n",
    "    )\n",
    "    for candidate in candidates_info:\n",
    "        cid = candidate['id']\n",
    "        title = candidate['title']\n",
    "        prompt += f\"{cid}: {title}\\n\"\n",
    "        prompt += \"\".join(IMAGE_TOKENS) + \"\\n\"\n",
    "    prompt += \"\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, data, item_meta, image_dir, candidate_type, max_conv_chars=None):\n",
    "        self.data = data\n",
    "        self.item_meta = item_meta\n",
    "        self.image_dir = image_dir\n",
    "        self.candidate_type = candidate_type\n",
    "        self.default_image = Image.new('RGB', (336, 336), color=(255, 255, 255))\n",
    "        if max_conv_chars is None:\n",
    "            max_conv_chars = MAX_LENGTH * CONV_CHARS_PER_TOKEN\n",
    "        self.max_conv_chars = max_conv_chars\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        conversation_text = entry.get('context', '')\n",
    "        gt_items = entry.get('gt_items', [])\n",
    "        candidates = entry.get(self.candidate_type, [])\n",
    "        candidate_info = prepare_candidate_info(candidates, self.item_meta, self.image_dir, self.default_image)\n",
    "        prompt = build_prompt(conversation_text, candidate_info, max_conv_chars=self.max_conv_chars)\n",
    "        images = [c['image'] for c in candidate_info]\n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'images': images,\n",
    "            'gt_items': gt_items,\n",
    "            'entry_idx': idx\n",
    "        }\n",
    "\n",
    "class DataCollatorForLLaVA:\n",
    "    def __init__(self, processor, tokenizer, max_length):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_token_ids = [\n",
    "            self.tokenizer.convert_tokens_to_ids(tk) for tk in IMAGE_TOKENS\n",
    "        ]\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        prompts = [item['prompt'] for item in batch]\n",
    "        images_per_sample = [item['images'] for item in batch]\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            prompts,\n",
    "            add_special_tokens=True,\n",
    "            padding=False,\n",
    "            truncation=False,\n",
    "        )\n",
    "\n",
    "        input_ids_list = []\n",
    "        for prompt_ids in tokenized['input_ids']:\n",
    "            prompt_ids = list(prompt_ids)\n",
    "            if len(prompt_ids) > self.max_length:\n",
    "                bos_id = self.tokenizer.bos_token_id\n",
    "                if bos_id is not None and prompt_ids[0] == bos_id:\n",
    "                    prompt_ids = [bos_id] + prompt_ids[-(self.max_length - 1):]\n",
    "                else:\n",
    "                    prompt_ids = prompt_ids[-self.max_length:]\n",
    "            input_ids_list.append(prompt_ids)\n",
    "\n",
    "        max_len = max(len(ids) for ids in input_ids_list)\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        input_ids = torch.full((len(input_ids_list), max_len), pad_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((len(input_ids_list), max_len), dtype=torch.long)\n",
    "\n",
    "        for i, ids in enumerate(input_ids_list):\n",
    "            seq_len = len(ids)\n",
    "            input_ids[i, :seq_len] = torch.tensor(ids, dtype=torch.long)\n",
    "            attention_mask[i, :seq_len] = 1\n",
    "\n",
    "        image_token_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        for b_idx in range(input_ids.size(0)):\n",
    "            for tid in self.image_token_ids:\n",
    "                positions = (input_ids[b_idx] == tid).nonzero(as_tuple=False).squeeze(-1)\n",
    "                image_token_mask[b_idx, positions] = True\n",
    "\n",
    "        all_images = []\n",
    "        for imgs in images_per_sample:\n",
    "            all_images.extend(imgs)\n",
    "\n",
    "        if all_images:\n",
    "            images_processed = self.processor.image_processor(all_images, return_tensors='pt')\n",
    "            images_tensor = images_processed['pixel_values']\n",
    "        else:\n",
    "            images_tensor = None\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'images': images_tensor,\n",
    "            'image_token_mask': image_token_mask\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e628df36-f7d5-4866-9325-bba960a9b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7446dca0294724b7fdd32b66b90e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load base model + processor\n",
    "base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=DTYPE,\n",
    "    local_files_only=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(BASE_MODEL_NAME, local_files_only=True)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": IMAGE_TOKENS})\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Merge vision adapter\n",
    "if VISION_ADAPTER_DIR and Path(VISION_ADAPTER_DIR).is_dir():\n",
    "    base_model = PeftModel.from_pretrained(base_model, VISION_ADAPTER_DIR, local_files_only=True)\n",
    "    base_model = base_model.merge_and_unload()\n",
    "    base_model.to(DEVICE)\n",
    "\n",
    "model = base_model\n",
    "if LM_ADAPTER_DIR and Path(LM_ADAPTER_DIR).is_dir():\n",
    "    model = PeftModel.from_pretrained(base_model, LM_ADAPTER_DIR, local_files_only=True)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Data\n",
    "item_meta = load_item_meta(ITEM_META_PATH)\n",
    "test_data = load_jsonl(TEST_DATA_PATH)\n",
    "test_ds = RecommendationDataset(test_data, item_meta, IMAGE_DIR, CANDIDATE_TYPE)\n",
    "collator = DataCollatorForLLaVA(processor, tokenizer, MAX_LENGTH)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb686f45-1d34-4928-ba47-77de224472b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ST_MODEL_NAME: /root/autodl-tmp/cache/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\n",
      "[DEBUG] Offline mode: HF_HUB_OFFLINE=1, TRANSFORMERS_OFFLINE=1\n",
      "[DEBUG] VISION_TEMPLATE: /root/autodl-tmp/lavic/src/out_distilled/{category}/lora_adapter_best\n",
      "[DEBUG] LM_TEMPLATE: /root/autodl-tmp/lavic/src/out_finetuned/{category}/prompt_tuning/trained_lora_adapter\n",
      "[DEBUG] Router categories: 26\n",
      "[DEBUG] ST model dim: 384\n",
      "[DEBUG] Router loaded: RouterMLPPlus\n",
      "[DEBUG] Adapter pairs found: 26\n",
      "[DEBUG] FALLBACK_CATEGORY: Appliances\n"
     ]
    }
   ],
   "source": [
    "# Batch evaluate all categories with the last cell's logic\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn as nn\n",
    "\n",
    "CATEGORY_LIST = [\n",
    "    \"amazon_home\",\n",
    "    \"amazon_fashion\",\n",
    "    \"all_beauty\",\n",
    "    \"Appliances\",\n",
    "    \"Arts_Crafts_and_Sewing\",\n",
    "    \"Automotive\",\n",
    "    \"Baby_Products\",\n",
    "    \"Books\",\n",
    "    \"CDs_and_Vinyl\",\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Digital_Music\",\n",
    "    \"Electronics\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Handmade_Products\",\n",
    "    \"Health\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Movies_and_TV\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\",\n",
    "    \"Pet_Supplies\",\n",
    "    \"Software\",\n",
    "    \"Sports_and_Outdoors\",\n",
    "    \"Toys_and_Games\",\n",
    "    \"Video_Games\",\n",
    "]\n",
    "\n",
    "# Force offline to avoid any downloads\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Gate router paths (from MoLoRA_Gating_lllm (1).ipynb)\n",
    "GATE_CKPT_DIR = Path(\"/root/autodl-tmp/cache/llmRouterMLPPlus\")\n",
    "GATE_ROUTER_PATH = GATE_CKPT_DIR / \"gate_router_mlp_plus.pt\"\n",
    "GATE_LABEL_MAP_PATH = GATE_CKPT_DIR / \"gate_label_mapping.json\"\n",
    "ST_MAX_LENGTH = 256\n",
    "ST_MODEL_NAME = \"/root/autodl-tmp/cache/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\"\n",
    "if not Path(ST_MODEL_NAME).is_dir():\n",
    "    raise FileNotFoundError(f\"ST model path not found: {ST_MODEL_NAME}\")\n",
    "print(\"[DEBUG] ST_MODEL_NAME:\", ST_MODEL_NAME)\n",
    "\n",
    "TOPK = 3\n",
    "CONF_THRESHOLD = 0.7  # set >0 to drop low-conf router picks\n",
    "FALLBACK_CATEGORY = None  # None => first available\n",
    "\n",
    "# Absolute path templates (CATEGORY will be replaced)\n",
    "VISION_TEMPLATE = \"/root/autodl-tmp/lavic/src/out_distilled/{category}/lora_adapter_best\"\n",
    "LM_TEMPLATE = \"/root/autodl-tmp/lavic/src/out_finetuned/{category}/prompt_tuning/trained_lora_adapter\"\n",
    "\n",
    "print(\"[DEBUG] Offline mode: HF_HUB_OFFLINE=1, TRANSFORMERS_OFFLINE=1\")\n",
    "print(\"[DEBUG] VISION_TEMPLATE:\", VISION_TEMPLATE)\n",
    "print(\"[DEBUG] LM_TEMPLATE:\", LM_TEMPLATE)\n",
    "\n",
    "class SimpleRouter(nn.Module):\n",
    "    def __init__(self, hidden_size, num_adapters):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(hidden_size, num_adapters))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "if BATCH_SIZE != 1:\n",
    "    raise ValueError(\"Top-3 router evaluation assumes BATCH_SIZE=1 (image placement).\")\n",
    "\n",
    "if not GATE_ROUTER_PATH.exists() or not GATE_LABEL_MAP_PATH.exists():\n",
    "    raise FileNotFoundError(\"Missing gate router or label mapping.\")\n",
    "\n",
    "with GATE_LABEL_MAP_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    label_map = json.load(f)\n",
    "sorted_indices = sorted(int(i) for i in label_map.keys())\n",
    "gate_categories = [label_map[str(i)] for i in sorted_indices]\n",
    "print(\"[DEBUG] Router categories:\", len(gate_categories))\n",
    "\n",
    "st_model = SentenceTransformer(ST_MODEL_NAME, device=DEVICE, cache_folder=os.environ.get(\"HF_HOME\"), local_files_only=True)\n",
    "st_model.max_seq_length = ST_MAX_LENGTH\n",
    "embed_dim = st_model.get_sentence_embedding_dimension()\n",
    "print(\"[DEBUG] ST model dim:\", embed_dim)\n",
    "\n",
    "class RouterMLPPlus(nn.Module):\n",
    "    def __init__(self, hidden_size, num_adapters, widths=(512, 256), dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, widths[0]),\n",
    "            nn.LayerNorm(widths[0]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(widths[0], widths[1]),\n",
    "            nn.LayerNorm(widths[1]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(widths[1], num_adapters)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "ROUTER_TYPE = \"RouterMLPPlus\"  # or \"SimpleRouter\"\n",
    "state = torch.load(GATE_ROUTER_PATH, map_location=DEVICE)\n",
    "if ROUTER_TYPE == \"RouterMLPPlus\":\n",
    "    router = RouterMLPPlus(hidden_size=embed_dim, num_adapters=len(gate_categories)).to(DEVICE).to(DTYPE)\n",
    "else:\n",
    "    router = SimpleRouter(hidden_size=embed_dim, num_adapters=len(gate_categories)).to(DEVICE).to(DTYPE)\n",
    "router.load_state_dict(state)\n",
    "router.eval()\n",
    "print(f\"[DEBUG] Router loaded: {ROUTER_TYPE}\")\n",
    "\n",
    "# Discover adapter pairs per category\n",
    "category_to_adapters = {}\n",
    "for cat in gate_categories:\n",
    "    v_dir = Path(VISION_TEMPLATE.format(category=cat))\n",
    "    l_dir = Path(LM_TEMPLATE.format(category=cat))\n",
    "    if v_dir.is_dir() and l_dir.is_dir():\n",
    "        category_to_adapters[cat] = {\n",
    "            \"vision\": str(v_dir),\n",
    "            \"lm\": str(l_dir),\n",
    "        }\n",
    "print(\"[DEBUG] Adapter pairs found:\", len(category_to_adapters))\n",
    "\n",
    "if not category_to_adapters:\n",
    "    raise RuntimeError(\"No valid (vision+lm) adapter pairs found.\")\n",
    "\n",
    "if FALLBACK_CATEGORY is None or FALLBACK_CATEGORY not in category_to_adapters:\n",
    "    FALLBACK_CATEGORY = sorted(category_to_adapters.keys())[0]\n",
    "print(\"[DEBUG] FALLBACK_CATEGORY:\", FALLBACK_CATEGORY)\n",
    "\n",
    "\n",
    "def evaluate_category(category: str) -> None:\n",
    "    global CATEGORY, ITEM_META_PATH, IMAGE_DIR, TEST_DATA_PATH, FINETUNE_OUTPUT_DIR\n",
    "\n",
    "    CATEGORY = category\n",
    "    ITEM_META_PATH = DATA_ROOT / f\"item2meta_train_{CATEGORY}.with_desc.json\"\n",
    "    IMAGE_DIR = DATA_ROOT / \"train_images\" / CATEGORY\n",
    "    TEST_DATA_PATH = DATA_ROOT / CATEGORY / \"test.jsonl\"\n",
    "    FINETUNE_OUTPUT_DIR = f\"./plora_test_out_v3/{CATEGORY}\"\n",
    "\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\"[DEBUG] Start category: {CATEGORY}\")\n",
    "    print(\"ITEM_META_PATH:\", ITEM_META_PATH)\n",
    "    print(\"IMAGE_DIR:\", IMAGE_DIR)\n",
    "    print(\"TEST_DATA_PATH:\", TEST_DATA_PATH)\n",
    "    print(\"FINETUNE_OUTPUT_DIR:\", FINETUNE_OUTPUT_DIR)\n",
    "    print(\"========================================\")\n",
    "\n",
    "    item_meta = load_item_meta(ITEM_META_PATH)\n",
    "    test_data = load_jsonl(TEST_DATA_PATH)\n",
    "    test_ds = RecommendationDataset(test_data, item_meta, IMAGE_DIR, CANDIDATE_TYPE)\n",
    "    collator = DataCollatorForLLaVA(processor, tokenizer, MAX_LENGTH)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "    # Pass 1: route top-3 for each sample\n",
    "    sample_top3 = []\n",
    "    category_to_samples = {cat: [] for cat in category_to_adapters.keys()}\n",
    "    empty_after_filter = 0\n",
    "    fallback_top1 = 0\n",
    "    fallback_fixed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=f\"Routing {CATEGORY}\", unit=\"batch\"), 1):\n",
    "            entry_idx = batch[0].get('entry_idx', None)\n",
    "            if entry_idx is None:\n",
    "                raise ValueError(\"Missing entry_idx in batch.\")\n",
    "            context_text = test_data[entry_idx].get('context', '').strip()\n",
    "            emb = st_model.encode([context_text], convert_to_tensor=True, device=DEVICE, show_progress_bar=False)\n",
    "            emb = emb.to(DTYPE)\n",
    "            logits = router(emb)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            top_probs, top_idx = torch.topk(probs, k=min(TOPK, probs.shape[-1]), dim=-1)\n",
    "\n",
    "            top_cats = []\n",
    "            top_confs = []\n",
    "            top1_idx = int(top_idx[0, 0].item())\n",
    "            top1_cat = label_map.get(str(top1_idx), None)\n",
    "            for i in range(top_idx.shape[-1]):\n",
    "                idx = int(top_idx[0, i].item())\n",
    "                cat = label_map.get(str(idx), None)\n",
    "                conf = float(top_probs[0, i].item())\n",
    "                if conf < CONF_THRESHOLD or cat not in category_to_adapters:\n",
    "                    continue\n",
    "                top_cats.append(cat)\n",
    "                top_confs.append(conf)\n",
    "\n",
    "            if not top_cats:\n",
    "                empty_after_filter += 1\n",
    "                if top1_cat in category_to_adapters:\n",
    "                    top_cats = [top1_cat]\n",
    "                    top_confs = [float(top_probs[0, 0].item())]\n",
    "                    fallback_top1 += 1\n",
    "                else:\n",
    "                    top_cats = [FALLBACK_CATEGORY]\n",
    "                    top_confs = [1.0]\n",
    "                    fallback_fixed += 1\n",
    "\n",
    "            sample_top3.append({\n",
    "                \"top_cats\": top_cats,\n",
    "                \"top_confs\": top_confs,\n",
    "            })\n",
    "            for cat in top_cats:\n",
    "                category_to_samples[cat].append(entry_idx)\n",
    "\n",
    "    print(f\"[DEBUG] Routing complete for {CATEGORY}. Samples: {len(sample_top3)}\")\n",
    "    print(f\"[DEBUG] empty_after_filter={empty_after_filter} fallback_top1={fallback_top1} fallback_fixed={fallback_fixed}\")\n",
    "\n",
    "    # Prepare aggregation containers\n",
    "    recommended_ids = [None] * len(test_data)\n",
    "    responses = [\"\"] * len(test_data)\n",
    "    ground_truths = [entry.get('gt_items', []) for entry in test_data]\n",
    "    selected_categories = [None] * len(test_data)\n",
    "    selected_confs = [None] * len(test_data)\n",
    "    selected_top3 = [None] * len(test_data)\n",
    "    selected_top3_confs = [None] * len(test_data)\n",
    "    selected_top3_recs = [None] * len(test_data)\n",
    "\n",
    "    score_maps = [None] * len(test_data)\n",
    "    resp_maps = [None] * len(test_data)\n",
    "\n",
    "    # Pass 2: process per category (load once)\n",
    "    for cat, sample_ids in category_to_samples.items():\n",
    "        if not sample_ids:\n",
    "            continue\n",
    "\n",
    "        v_dir = category_to_adapters[cat][\"vision\"]\n",
    "        l_dir = category_to_adapters[cat][\"lm\"]\n",
    "        print(f\"[DEBUG] Load model for {cat}:\\n  vision={v_dir}\\n  lm={l_dir}\\n  samples={len(sample_ids)}\")\n",
    "\n",
    "        base = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            BASE_MODEL_NAME,\n",
    "            torch_dtype=DTYPE,\n",
    "            local_files_only=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # merge vision adapter\n",
    "        base = PeftModel.from_pretrained(base, v_dir, local_files_only=True)\n",
    "        base = base.merge_and_unload()\n",
    "        base = base.to(DEVICE)\n",
    "\n",
    "        # apply LM adapter\n",
    "        model = PeftModel.from_pretrained(base, l_dir, local_files_only=True).to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for entry_idx in tqdm(sample_ids, desc=f\"Evaluating {CATEGORY}->{cat}\", unit=\"sample\"):\n",
    "                batch = [test_ds[entry_idx]]\n",
    "                inputs = collator(batch)\n",
    "                input_ids = inputs['input_ids'].to(DEVICE)\n",
    "                attention_mask = inputs['attention_mask'].to(DEVICE)\n",
    "                image_token_mask = inputs['image_token_mask'].to(DEVICE)\n",
    "                images = inputs['images']\n",
    "\n",
    "                inputs_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "                if images is not None:\n",
    "                    images = images.to(DEVICE, dtype=DTYPE)\n",
    "                    if images.dim() == 5:\n",
    "                        b_img, num_views, c, h, w = images.shape\n",
    "                        images_reshaped = images.view(-1, c, h, w)\n",
    "                    else:\n",
    "                        num_views = 1\n",
    "                        images_reshaped = images\n",
    "                    vision_outputs = model.vision_tower(images_reshaped, return_dict=False)\n",
    "                    cls_states = vision_outputs[0][:, 0, :]\n",
    "                    total_patches = cls_states.shape[0]\n",
    "                    expected_images = input_ids.size(0) * num_views\n",
    "                    if total_patches > expected_images:\n",
    "                        patches_per_image = total_patches // expected_images\n",
    "                        if total_patches % expected_images == 0 and patches_per_image > 1:\n",
    "                            cls_states = cls_states.view(expected_images, patches_per_image, -1).mean(dim=1)\n",
    "                    cls_states = cls_states.reshape(input_ids.size(0), num_views, -1)\n",
    "                    cls_states = model.multi_modal_projector(cls_states)\n",
    "                    for b_idx in range(input_ids.size(0)):\n",
    "                        positions = torch.nonzero(image_token_mask[b_idx], as_tuple=False).squeeze(-1)\n",
    "                        pos_count = min(len(positions), num_views)\n",
    "                        for i in range(pos_count):\n",
    "                            col = positions[i].item()\n",
    "                            inputs_embeds[b_idx, col, :] = cls_states[b_idx, i, :]\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=10,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                )\n",
    "                generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                match = re.findall(r'\\bB[A-Z0-9]{9}\\b', generated_texts[0].strip())\n",
    "                rec_id = match[0][:10] if match else None\n",
    "\n",
    "                # init per-sample aggregation\n",
    "                if score_maps[entry_idx] is None:\n",
    "                    score_maps[entry_idx] = {}\n",
    "                    resp_maps[entry_idx] = {}\n",
    "                    selected_top3[entry_idx] = sample_top3[entry_idx][\"top_cats\"]\n",
    "                    selected_top3_confs[entry_idx] = sample_top3[entry_idx][\"top_confs\"]\n",
    "                    selected_categories[entry_idx] = sample_top3[entry_idx][\"top_cats\"][0]\n",
    "                    selected_confs[entry_idx] = sample_top3[entry_idx][\"top_confs\"][0]\n",
    "\n",
    "                # weight by router confidence for this category\n",
    "                top_cats = sample_top3[entry_idx][\"top_cats\"]\n",
    "                top_confs = sample_top3[entry_idx][\"top_confs\"]\n",
    "                conf = top_confs[top_cats.index(cat)] if cat in top_cats else 0.0\n",
    "\n",
    "                score_maps[entry_idx][rec_id] = score_maps[entry_idx].get(rec_id, 0.0) + conf\n",
    "                if rec_id not in resp_maps[entry_idx]:\n",
    "                    resp_maps[entry_idx][rec_id] = generated_texts[0]\n",
    "\n",
    "        del model\n",
    "        del base\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Final fusion per sample\n",
    "    for i in range(len(test_data)):\n",
    "        if score_maps[i] is None or not score_maps[i]:\n",
    "            recommended_ids[i] = None\n",
    "            responses[i] = \"\"\n",
    "            selected_top3_recs[i] = []\n",
    "            continue\n",
    "\n",
    "        best_rec_id = max(score_maps[i].items(), key=lambda kv: kv[1])[0]\n",
    "        recommended_ids[i] = best_rec_id\n",
    "        responses[i] = resp_maps[i].get(best_rec_id, \"\")\n",
    "        selected_top3_recs[i] = [None] * len(selected_top3[i])\n",
    "\n",
    "    recall = evaluate_recall_at_k(recommended_ids, ground_truths, k=1)\n",
    "    print(f\"[Test] Recall@1 ({CATEGORY}): {recall:.4f}\")\n",
    "\n",
    "    model_key = 'st' if CANDIDATE_TYPE == 'candidates_st' else 'gpt_large'\n",
    "    if model_key == 'st':\n",
    "        recommended_field = 'recommended_st'\n",
    "        response_field = 'response_st'\n",
    "    else:\n",
    "        recommended_field = f\"recommended_{model_key}\"\n",
    "        response_field = f\"response_{model_key}\"\n",
    "\n",
    "    out_file_name = f\"test_results_{CANDIDATE_TYPE}_router_top3_vl.jsonl\"\n",
    "    output_file_path = Path(FINETUNE_OUTPUT_DIR) / out_file_name\n",
    "    output_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with output_file_path.open('w', encoding='utf-8') as f:\n",
    "        for entry, rec_id, resp, sel_cat, sel_conf, top3, top3_confs in zip(\n",
    "            test_data, recommended_ids, responses, selected_categories, selected_confs,\n",
    "            selected_top3, selected_top3_confs\n",
    "        ):\n",
    "            entry[recommended_field] = rec_id\n",
    "            entry[response_field] = resp\n",
    "            entry['selected_category'] = sel_cat\n",
    "            entry['selected_confidence'] = sel_conf\n",
    "            entry['router_top3'] = top3\n",
    "            entry['router_top3_confidences'] = top3_confs\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "    validity, invalid_entries, total_count = check_validity(output_file_path, model_key)\n",
    "    print(f\"Validity@1 ({CATEGORY}): {validity:.4f}, invalid entries: {len(invalid_entries)} / {total_count}\")\n",
    "    print(f\"Test details saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ccd9f3-924a-4930-9ea9-83b6768d1fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] category=Books samples=200\n",
      "[DEBUG] pmax mean=0.7801 median=0.8191 min=0.3184 max=0.9136\n",
      "[DEBUG] empty_after_filter=10/200 (CONF_THRESHOLD=0.5, TOPK=3)\n",
      "[DEBUG] pmax histogram bins=10:\n",
      "  0.0-0.1: 0\n",
      "  0.1-0.2: 0\n",
      "  0.2-0.3: 0\n",
      "  0.3-0.4: 7\n",
      "  0.4-0.5: 3\n",
      "  0.5-0.6: 9\n",
      "  0.6-0.7: 18\n",
      "  0.7-0.8: 40\n",
      "  0.8-0.9: 116\n",
      "  0.9-1.0: 7\n"
     ]
    }
   ],
   "source": [
    "# Debug router confidence distribution for a single category\n",
    "import numpy as np\n",
    "\n",
    "debug_category = \"Books\"  # change if needed\n",
    "max_samples = 200  # limit for quick debug\n",
    "\n",
    "if 'category_to_adapters' not in globals():\n",
    "    raise RuntimeError(\"category_to_adapters not found. Run the setup cell first.\")\n",
    "\n",
    "ITEM_META_PATH = DATA_ROOT / f\"item2meta_train_{debug_category}.with_desc.json\"\n",
    "TEST_DATA_PATH = DATA_ROOT / debug_category / \"test.jsonl\"\n",
    "\n",
    "test_data = load_jsonl(TEST_DATA_PATH)\n",
    "\n",
    "pmax_list = []\n",
    "empty_after_filter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, entry in enumerate(test_data[:max_samples]):\n",
    "        context_text = entry.get('context', '').strip()\n",
    "        emb = st_model.encode([context_text], convert_to_tensor=True, device=DEVICE, show_progress_bar=False)\n",
    "        emb = emb.to(DTYPE)\n",
    "        logits = router(emb)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pmax, imax = probs.max(dim=-1)\n",
    "        pmax_list.append(float(pmax.item()))\n",
    "\n",
    "        top_probs, top_idx = torch.topk(probs, k=min(TOPK, probs.shape[-1]), dim=-1)\n",
    "        top_cats = []\n",
    "        for j in range(top_idx.shape[-1]):\n",
    "            idx = int(top_idx[0, j].item())\n",
    "            cat = label_map.get(str(idx), None)\n",
    "            conf = float(top_probs[0, j].item())\n",
    "            if conf < CONF_THRESHOLD or cat not in category_to_adapters:\n",
    "                continue\n",
    "            top_cats.append(cat)\n",
    "        if not top_cats:\n",
    "            empty_after_filter += 1\n",
    "\n",
    "pmax_arr = np.array(pmax_list, dtype=float)\n",
    "print(f\"[DEBUG] category={debug_category} samples={len(pmax_arr)}\")\n",
    "print(f\"[DEBUG] pmax mean={pmax_arr.mean():.4f} median={np.median(pmax_arr):.4f} min={pmax_arr.min():.4f} max={pmax_arr.max():.4f}\")\n",
    "print(f\"[DEBUG] empty_after_filter={empty_after_filter}/{len(pmax_arr)} (CONF_THRESHOLD={CONF_THRESHOLD}, TOPK={TOPK})\")\n",
    "print(\"[DEBUG] pmax histogram bins=10:\")\n",
    "counts, bins = np.histogram(pmax_arr, bins=10, range=(0.0, 1.0))\n",
    "for c, b0, b1 in zip(counts, bins[:-1], bins[1:]):\n",
    "    print(f\"  {b0:.1f}-{b1:.1f}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6732dbe-b68e-46cd-b244-5e5220784279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Automotive\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Automotive.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Automotive\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Automotive/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Automotive\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Automotive: 100%|██████████| 152/152 [00:02<00:00, 73.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Automotive. Samples: 152\n",
      "[DEBUG] empty_after_filter=22 fallback_top1=22 fallback_fixed=0\n",
      "[DEBUG] Load model for Arts_Crafts_and_Sewing:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Arts_Crafts_and_Sewing/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Arts_Crafts_and_Sewing/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b448c5063e294ba8928acb9ad97df8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Automotive->Arts_Crafts_and_Sewing: 100%|██████████| 2/2 [00:01<00:00,  1.51sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Automotive:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Automotive/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "  samples=138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a073b2d3fd5c4322ab755e3b03f57ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_category\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAutomotive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mevaluate_category\u001b[39m\u001b[34m(category)\u001b[39m\n\u001b[32m    228\u001b[39m l_dir = category_to_adapters[cat][\u001b[33m\"\u001b[39m\u001b[33mlm\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    229\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] Load model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  vision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  lm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m base = \u001b[43mLlavaNextForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBASE_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# merge vision adapter\u001b[39;00m\n\u001b[32m    238\u001b[39m base = PeftModel.from_pretrained(base, v_dir, local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 928 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "evaluate_category(\"Automotive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fce6da00-94d2-4e63-8ac4-a5b54654deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: all_beauty\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_all_beauty.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/all_beauty\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/all_beauty/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/all_beauty\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing all_beauty: 100%|██████████| 772/772 [00:10<00:00, 74.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for all_beauty. Samples: 772\n",
      "[DEBUG] Load model for Arts_Crafts_and_Sewing:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Arts_Crafts_and_Sewing/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Arts_Crafts_and_Sewing/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd4d46bea4493eae8bd58a0b7edb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Arts_Crafts_and_Sewing: 100%|██████████| 2/2 [00:01<00:00,  1.48sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Automotive:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Automotive/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d973f9e0a794e609759e03c5df39b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Automotive: 100%|██████████| 4/4 [00:02<00:00,  1.58sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Baby_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Baby_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Baby_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da4f1c661fe44189c9ccc9a8afabebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Baby_Products: 100%|██████████| 5/5 [00:03<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7df95f78965478fa590d23f3953a338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Books: 100%|██████████| 1/1 [00:00<00:00,  1.42sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f6605bad5548b38ed27da6a482aa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->CDs_and_Vinyl: 100%|██████████| 2/2 [00:01<00:00,  1.61sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Cell_Phones_and_Accessories:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Cell_Phones_and_Accessories/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Cell_Phones_and_Accessories/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f2b09ab9df4d2cafaa68b4fe536c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Cell_Phones_and_Accessories: 100%|██████████| 1/1 [00:00<00:00,  1.47sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Grocery_and_Gourmet_Food:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Grocery_and_Gourmet_Food/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Grocery_and_Gourmet_Food/prompt_tuning/trained_lora_adapter\n",
      "  samples=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3b03269acb4d11973664da3feb706a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Grocery_and_Gourmet_Food: 100%|██████████| 8/8 [00:05<00:00,  1.50sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Health:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Health/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Health/prompt_tuning/trained_lora_adapter\n",
      "  samples=31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c25553e41a74c61b9973e723cb65d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Health: 100%|██████████| 31/31 [00:20<00:00,  1.50sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Industrial_and_Scientific:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Industrial_and_Scientific/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Industrial_and_Scientific/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3aaf9b25bf4ad49feec84b3f065f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Industrial_and_Scientific: 100%|██████████| 2/2 [00:01<00:00,  1.48sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Movies_and_TV:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Movies_and_TV/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Movies_and_TV/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588287f861c14bb7bc32de1416014d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Movies_and_TV: 100%|██████████| 3/3 [00:01<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Musical_Instruments:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Musical_Instruments/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Musical_Instruments/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c1098a562d4504a6de7a6ab9a85a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Musical_Instruments: 100%|██████████| 1/1 [00:00<00:00,  1.51sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Patio_Lawn_and_Garden:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Patio_Lawn_and_Garden/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Patio_Lawn_and_Garden/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26aa97b781247ae97a0be68a4e06cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Patio_Lawn_and_Garden: 100%|██████████| 3/3 [00:02<00:00,  1.46sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Pet_Supplies:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Pet_Supplies/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Pet_Supplies/prompt_tuning/trained_lora_adapter\n",
      "  samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06211cc57dff487abba82d60348397c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Pet_Supplies: 100%|██████████| 10/10 [00:06<00:00,  1.52sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Sports_and_Outdoors:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Sports_and_Outdoors/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Sports_and_Outdoors/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5d3f305ed648ecb854ee8842d534e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Sports_and_Outdoors: 100%|██████████| 5/5 [00:03<00:00,  1.50sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Toys_and_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Toys_and_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Toys_and_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf064e966c0426297a12612ba699cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Toys_and_Games: 100%|██████████| 2/2 [00:01<00:00,  1.46sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Video_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Video_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Video_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770354017b6241a6af398e0154865c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->Video_Games: 100%|██████████| 1/1 [00:00<00:00,  1.49sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for all_beauty:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/all_beauty/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/all_beauty/prompt_tuning/trained_lora_adapter\n",
      "  samples=624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6632715acd0c495f8e54d433e7fcf742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->all_beauty: 100%|██████████| 624/624 [06:32<00:00,  1.59sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_fashion:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_fashion/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_fashion/prompt_tuning/trained_lora_adapter\n",
      "  samples=38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472f5938b7f84426acc4a6e4b6c71d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->amazon_fashion: 100%|██████████| 38/38 [00:23<00:00,  1.59sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa79e7375256484ba2e4bd2bfe3e067f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating all_beauty->amazon_home: 100%|██████████| 29/29 [00:19<00:00,  1.52sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (all_beauty): 0.1000\n",
      "Validity@1 (all_beauty): 0.9909, invalid entries: 7 / 772\n",
      "Test details saved to plora_test_out_v3/all_beauty/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_category(\"all_beauty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f9c00bc-1031-4e33-9754-03bedc789fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: amazon_fashion\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_amazon_fashion.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/amazon_fashion\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/amazon_fashion/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/amazon_fashion\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing amazon_fashion: 100%|██████████| 826/826 [00:09<00:00, 89.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for amazon_fashion. Samples: 826\n",
      "[DEBUG] Load model for Arts_Crafts_and_Sewing:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Arts_Crafts_and_Sewing/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Arts_Crafts_and_Sewing/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9028e93caa4dee9bd1c190715215b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Arts_Crafts_and_Sewing: 100%|██████████| 3/3 [00:01<00:00,  1.64sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Automotive:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Automotive/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253a1db571544082a3344e307cdb7073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Automotive: 100%|██████████| 3/3 [00:01<00:00,  1.59sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Baby_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Baby_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Baby_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef936aefe2c4288ab5f149e5f4512be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Baby_Products: 100%|██████████| 10/10 [00:05<00:00,  1.68sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb6923c692649eb89048c889e89b200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->CDs_and_Vinyl: 100%|██████████| 3/3 [00:01<00:00,  1.70sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Cell_Phones_and_Accessories:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Cell_Phones_and_Accessories/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Cell_Phones_and_Accessories/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2cb4ef4fcb4e469c94d06aafb0dfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Cell_Phones_and_Accessories: 100%|██████████| 5/5 [00:03<00:00,  1.60sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Digital_Music:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Digital_Music/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Digital_Music/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be27e8fe0fcb437ab605c47e2daa5013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Digital_Music: 100%|██████████| 1/1 [00:00<00:00,  1.50sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Electronics:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Electronics/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Electronics/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92a310dfd9945fb9573792c13562af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Electronics: 100%|██████████| 4/4 [00:02<00:00,  1.60sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Grocery_and_Gourmet_Food:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Grocery_and_Gourmet_Food/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Grocery_and_Gourmet_Food/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194ad9d022944fb6ae427e8a62f8c3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Grocery_and_Gourmet_Food: 100%|██████████| 2/2 [00:01<00:00,  1.61sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Handmade_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Handmade_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Handmade_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886e5dd492f8479e8af692d2d9696758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Handmade_Products: 100%|██████████| 13/13 [00:08<00:00,  1.62sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Health:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Health/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Health/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af831ad24504112b0f38d9abbd4128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Health: 100%|██████████| 4/4 [00:02<00:00,  1.59sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Industrial_and_Scientific:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Industrial_and_Scientific/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Industrial_and_Scientific/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc395b4591be4aa581fe44f4b7efecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Industrial_and_Scientific: 100%|██████████| 1/1 [00:00<00:00,  1.60sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Movies_and_TV:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Movies_and_TV/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Movies_and_TV/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4c56c14b464376aa6fffb43fb5a1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Movies_and_TV: 100%|██████████| 2/2 [00:01<00:00,  1.78sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Musical_Instruments:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Musical_Instruments/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Musical_Instruments/prompt_tuning/trained_lora_adapter\n",
      "  samples=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b227f4f92c45f3944f45f783614f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Musical_Instruments: 100%|██████████| 6/6 [00:03<00:00,  1.60sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Office_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Office_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Office_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1193c2572f8d48c29b4319444c26bbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Office_Products: 100%|██████████| 5/5 [00:02<00:00,  1.69sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Patio_Lawn_and_Garden:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Patio_Lawn_and_Garden/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Patio_Lawn_and_Garden/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f71812817e485e8f9c0ab9ea28528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Patio_Lawn_and_Garden: 100%|██████████| 1/1 [00:00<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Pet_Supplies:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Pet_Supplies/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Pet_Supplies/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7afc004d8624fbcacd3404671a63910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Pet_Supplies: 100%|██████████| 2/2 [00:01<00:00,  1.56sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Software:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Software/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Software/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b113173add4e098a57fc2bc9f20174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Software: 100%|██████████| 1/1 [00:00<00:00,  1.30sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Sports_and_Outdoors:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Sports_and_Outdoors/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Sports_and_Outdoors/prompt_tuning/trained_lora_adapter\n",
      "  samples=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1872d95270ce4f3f8a9ed4dffa55be08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Sports_and_Outdoors: 100%|██████████| 18/18 [00:11<00:00,  1.58sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Toys_and_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Toys_and_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Toys_and_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7184e9c3ae7b4ffdaee1a51b521c9a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Toys_and_Games: 100%|██████████| 4/4 [00:02<00:00,  1.65sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Video_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Video_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Video_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0713262a41497c91baf6257b0821b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->Video_Games: 100%|██████████| 3/3 [00:01<00:00,  1.64sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for all_beauty:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/all_beauty/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/all_beauty/prompt_tuning/trained_lora_adapter\n",
      "  samples=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29226cbf3b4de2a5b22d76feaf2e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->all_beauty: 100%|██████████| 36/36 [00:22<00:00,  1.61sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_fashion:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_fashion/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_fashion/prompt_tuning/trained_lora_adapter\n",
      "  samples=658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315454b3c1d24547bdbf0f302b68441c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->amazon_fashion: 100%|██████████| 658/658 [06:30<00:00,  1.69sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75a320ec2a740828ae36144f2e8acff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_fashion->amazon_home: 100%|██████████| 41/41 [00:25<00:00,  1.60sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (amazon_fashion): 0.1072\n",
      "Validity@1 (amazon_fashion): 0.9915, invalid entries: 7 / 826\n",
      "Test details saved to plora_test_out_v3/amazon_fashion/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_category(\"amazon_fashion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b53a8d4-eed7-4f4b-9759-5272f1736d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: amazon_home\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_amazon_home.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/amazon_home\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/amazon_home/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/amazon_home\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing amazon_home: 100%|██████████| 372/372 [00:04<00:00, 84.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for amazon_home. Samples: 372\n",
      "[DEBUG] Load model for Appliances:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Appliances/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Appliances/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df339e0a34d40859300bb1658d917c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Appliances: 100%|██████████| 3/3 [00:01<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Arts_Crafts_and_Sewing:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Arts_Crafts_and_Sewing/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Arts_Crafts_and_Sewing/prompt_tuning/trained_lora_adapter\n",
      "  samples=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182b54f6fd2414cb62858c28d5c91c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Arts_Crafts_and_Sewing: 100%|██████████| 12/12 [00:07<00:00,  1.53sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Automotive:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Automotive/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "  samples=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef12c65c32c4e20a82d1eff8e9d192a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Automotive: 100%|██████████| 11/11 [00:06<00:00,  1.63sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Baby_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Baby_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Baby_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e127732e02e4764a28fd7c289da8208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Baby_Products: 100%|██████████| 5/5 [00:03<00:00,  1.53sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10de3d6ecfa74b12ba37cc5fc07ed907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Books: 100%|██████████| 3/3 [00:01<00:00,  1.62sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533d3a89dc0448489e46d74e15995635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->CDs_and_Vinyl: 100%|██████████| 2/2 [00:01<00:00,  1.53sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Cell_Phones_and_Accessories:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Cell_Phones_and_Accessories/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Cell_Phones_and_Accessories/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87027d410eff49218514da4bf20b92de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Cell_Phones_and_Accessories: 100%|██████████| 2/2 [00:01<00:00,  1.55sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Electronics:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Electronics/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Electronics/prompt_tuning/trained_lora_adapter\n",
      "  samples=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2179a6fec8434886440d4e1655e7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Electronics: 100%|██████████| 11/11 [00:07<00:00,  1.55sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Grocery_and_Gourmet_Food:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Grocery_and_Gourmet_Food/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Grocery_and_Gourmet_Food/prompt_tuning/trained_lora_adapter\n",
      "  samples=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee22c163e5a94fa89668a1db230dbe11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Grocery_and_Gourmet_Food: 100%|██████████| 11/11 [00:06<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Handmade_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Handmade_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Handmade_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8503c879e4004a6591697edb81fdc504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Handmade_Products: 100%|██████████| 5/5 [00:03<00:00,  1.54sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Health:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Health/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Health/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baca2e47f71244b5b674bad7e2f50d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Health: 100%|██████████| 4/4 [00:02<00:00,  1.48sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Industrial_and_Scientific:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Industrial_and_Scientific/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Industrial_and_Scientific/prompt_tuning/trained_lora_adapter\n",
      "  samples=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5349f737a5814638be23d9ceef9ba0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Industrial_and_Scientific: 100%|██████████| 11/11 [00:06<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Musical_Instruments:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Musical_Instruments/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Musical_Instruments/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83afc5474551409eb43169d4d150014e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Musical_Instruments: 100%|██████████| 3/3 [00:01<00:00,  1.66sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Office_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Office_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Office_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770f905c36a34f3b8619b39eb72f81c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Office_Products: 100%|██████████| 9/9 [00:05<00:00,  1.61sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Patio_Lawn_and_Garden:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Patio_Lawn_and_Garden/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Patio_Lawn_and_Garden/prompt_tuning/trained_lora_adapter\n",
      "  samples=8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29ef88611d64207b200464471701976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Patio_Lawn_and_Garden: 100%|██████████| 8/8 [00:05<00:00,  1.56sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Pet_Supplies:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Pet_Supplies/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Pet_Supplies/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104ebeff3b17444b83f521655d43c375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Pet_Supplies: 100%|██████████| 5/5 [00:03<00:00,  1.50sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Sports_and_Outdoors:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Sports_and_Outdoors/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Sports_and_Outdoors/prompt_tuning/trained_lora_adapter\n",
      "  samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44741f1866ce470e9b135b016993c7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Sports_and_Outdoors: 100%|██████████| 5/5 [00:03<00:00,  1.58sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Toys_and_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Toys_and_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Toys_and_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f157588fe504a71a76ef22ce70dc887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->Toys_and_Games: 100%|██████████| 4/4 [00:02<00:00,  1.58sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for all_beauty:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/all_beauty/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/all_beauty/prompt_tuning/trained_lora_adapter\n",
      "  samples=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59c74ab2e094c87a218f1274e3e1514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->all_beauty: 100%|██████████| 20/20 [00:12<00:00,  1.55sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_fashion:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_fashion/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_fashion/prompt_tuning/trained_lora_adapter\n",
      "  samples=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9256662a4af94cc58ad0df306baf90c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->amazon_fashion: 100%|██████████| 20/20 [00:12<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7fbacdf03844a19633ae98ed3ca86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating amazon_home->amazon_home: 100%|██████████| 218/218 [02:17<00:00,  1.58sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (amazon_home): 0.1990\n",
      "Validity@1 (amazon_home): 1.0000, invalid entries: 0 / 372\n",
      "Test details saved to plora_test_out_v3/amazon_home/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_category(\"amazon_home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b27b936-4e1a-438d-b1d2-98b14f059479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Books\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Books.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Books\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Books/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Books\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Books: 100%|██████████| 472/472 [00:06<00:00, 71.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Books. Samples: 472\n",
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fa25ee7e3f4ce59231395c2278fdcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Books: 100%|██████████| 448/448 [05:36<00:00,  1.33sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5d0a2a218f4e94b9ba3e181416b5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->CDs_and_Vinyl: 100%|██████████| 2/2 [00:01<00:00,  1.13sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Electronics:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Electronics/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Electronics/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcd99f6f87f4922a54e5a99e27c6157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Electronics: 100%|██████████| 1/1 [00:00<00:00,  1.36sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Grocery_and_Gourmet_Food:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Grocery_and_Gourmet_Food/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Grocery_and_Gourmet_Food/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fff2dd02384491a46b253fa9d95161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Grocery_and_Gourmet_Food: 100%|██████████| 2/2 [00:01<00:00,  1.26sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Handmade_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Handmade_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Handmade_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933719e558754063b56e86951dfe7a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Handmade_Products: 100%|██████████| 1/1 [00:00<00:00,  1.07sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Kindle_Store:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Kindle_Store/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Kindle_Store/prompt_tuning/trained_lora_adapter\n",
      "  samples=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48db07aae3e40a9ae137435766acafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Kindle_Store: 100%|██████████| 15/15 [00:11<00:00,  1.29sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Musical_Instruments:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Musical_Instruments/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Musical_Instruments/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1aa5246a09242158b43b60395b40c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Musical_Instruments: 100%|██████████| 1/1 [00:00<00:00,  1.13sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Patio_Lawn_and_Garden:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Patio_Lawn_and_Garden/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Patio_Lawn_and_Garden/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c96c894bc0491b86b72e0c1b4ba564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Patio_Lawn_and_Garden: 100%|██████████| 1/1 [00:00<00:00,  1.19sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_fashion:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_fashion/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_fashion/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5417c46371414ab9cd7d5ef798a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->amazon_fashion: 100%|██████████| 1/1 [00:00<00:00,  1.14sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (Books): 0.0318\n",
      "Validity@1 (Books): 0.2881, invalid entries: 336 / 472\n",
      "Test details saved to plora_test_out_v3/Books/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run with CONF_THRESHOLD = 0.9\n",
    "evaluate_category(\"Books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9e9b380-5769-4983-85dd-02e7871551fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Toys_and_Games\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Toys_and_Games.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Toys_and_Games\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Toys_and_Games/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Toys_and_Games\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Toys_and_Games: 100%|██████████| 192/192 [00:03<00:00, 58.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Toys_and_Games. Samples: 192\n",
      "[DEBUG] Load model for Automotive:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Automotive/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a9fd9b244248fcbef4cfdec231f0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Automotive: 100%|██████████| 1/1 [00:00<00:00,  1.10sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da43ed40e5e142a59dc4ca6cef377192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Books: 100%|██████████| 1/1 [00:00<00:00,  1.24sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1582a12b0b44b67984821a2a9565162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->CDs_and_Vinyl: 100%|██████████| 1/1 [00:00<00:00,  1.28sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Cell_Phones_and_Accessories:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Cell_Phones_and_Accessories/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Cell_Phones_and_Accessories/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97320aa85434471db83c4f96d5114d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Cell_Phones_and_Accessories: 100%|██████████| 1/1 [00:00<00:00,  1.20sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Handmade_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Handmade_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Handmade_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0abe99c49014560aa81241a21b0266d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Handmade_Products: 100%|██████████| 2/2 [00:01<00:00,  1.08sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Movies_and_TV:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Movies_and_TV/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Movies_and_TV/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c815c78bb6284afea605cf02f750363a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Movies_and_TV: 100%|██████████| 3/3 [00:02<00:00,  1.15sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Toys_and_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Toys_and_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Toys_and_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b28ee4f902745ba953577fa98f384cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Toys_and_Games: 100%|██████████| 176/176 [02:26<00:00,  1.20sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Video_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Video_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Video_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb23dbe39ef34bce8bf87c4d804344e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Video_Games: 100%|██████████| 4/4 [00:02<00:00,  1.35sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dc46a77fd44a1f8d3ff96237365290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->amazon_home: 100%|██████████| 3/3 [00:02<00:00,  1.20sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (Toys_and_Games): 0.3177\n",
      "Validity@1 (Toys_and_Games): 1.0000, invalid entries: 0 / 192\n",
      "Test details saved to plora_test_out_v3/Toys_and_Games/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run with CONF_THRESHOLD = 0.9\n",
    "evaluate_category(\"Toys_and_Games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3e99b2a-59dc-49dc-9287-d19a5bd8af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Appliances\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Appliances.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Appliances\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Appliances/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Appliances\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Appliances: 100%|██████████| 230/230 [00:03<00:00, 72.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Appliances. Samples: 230\n",
      "[DEBUG] Load model for Appliances:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Appliances/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Appliances/prompt_tuning/trained_lora_adapter\n",
      "  samples=222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33d5dec78014c6f92aab6a8bf3ea634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->Appliances: 100%|██████████| 222/222 [02:44<00:00,  1.35sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Pet_Supplies:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Pet_Supplies/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Pet_Supplies/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8d2a03d6eb44bd815a951bc8f92ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->Pet_Supplies: 100%|██████████| 1/1 [00:00<00:00,  1.10sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Software:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Software/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Software/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748bb2253dba44e588d2fe6fadda4d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->Software: 100%|██████████| 1/1 [00:00<00:00,  1.03sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86826265e1374f3bb4518f6365ab4d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->amazon_home: 100%|██████████| 6/6 [00:03<00:00,  1.57sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (Appliances): 0.3522\n",
      "Validity@1 (Appliances): 0.9957, invalid entries: 1 / 230\n",
      "Test details saved to plora_test_out_v3/Appliances/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run with CONF_THRESHOLD = 0.9\n",
    "evaluate_category(\"Appliances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b04548b0-698b-4ca8-92eb-4c303a1f3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Books\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Books.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Books\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Books/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Books\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Books: 100%|██████████| 472/472 [00:06<00:00, 72.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Books. Samples: 472\n",
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e95c0f4b45e4e3f9d2e8f4b7dd150ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Books: 100%|██████████| 448/448 [05:59<00:00,  1.25sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a73a56a9f64587b3505ab1c92e05cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->CDs_and_Vinyl: 100%|██████████| 2/2 [00:01<00:00,  1.34sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Electronics:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Electronics/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Electronics/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4c17c959fd4116a8950a8212430e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Electronics: 100%|██████████| 1/1 [00:00<00:00,  1.49sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Grocery_and_Gourmet_Food:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Grocery_and_Gourmet_Food/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Grocery_and_Gourmet_Food/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca05146b1e65409e822f2f11a5de4e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Grocery_and_Gourmet_Food: 100%|██████████| 2/2 [00:01<00:00,  1.54sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Handmade_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Handmade_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Handmade_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a63257179a248a8ad6099fa1f45fc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Handmade_Products: 100%|██████████| 1/1 [00:00<00:00,  1.16sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Kindle_Store:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Kindle_Store/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Kindle_Store/prompt_tuning/trained_lora_adapter\n",
      "  samples=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebadfd13242d45a8b35803e7dbc5a107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Kindle_Store: 100%|██████████| 15/15 [00:11<00:00,  1.31sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Musical_Instruments:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Musical_Instruments/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Musical_Instruments/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2facabd547cd454bb49eeb6c08bdd150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Musical_Instruments: 100%|██████████| 1/1 [00:00<00:00,  1.17sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Patio_Lawn_and_Garden:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Patio_Lawn_and_Garden/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Patio_Lawn_and_Garden/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f079c18451da48968f92ebca182f4d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->Patio_Lawn_and_Garden: 100%|██████████| 1/1 [00:00<00:00,  1.21sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_fashion:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_fashion/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_fashion/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12204619d174468acda5ae529b01500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Books->amazon_fashion: 100%|██████████| 1/1 [00:00<00:00,  1.07sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (Books): 0.0318\n",
      "Validity@1 (Books): 0.2881, invalid entries: 336 / 472\n",
      "Test details saved to plora_test_out_v3/Books/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run with CONF_THRESHOLD = 0.5\n",
    "evaluate_category(\"Books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4aee08bd-8520-4f71-9ebd-58b89d72af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Toys_and_Games\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Toys_and_Games.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Toys_and_Games\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Toys_and_Games/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Toys_and_Games\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Toys_and_Games: 100%|██████████| 192/192 [00:03<00:00, 53.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Toys_and_Games. Samples: 192\n",
      "[DEBUG] Load model for Automotive:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Automotive/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Automotive/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e2821040484958ba18ce20b7ba105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Automotive: 100%|██████████| 1/1 [00:00<00:00,  1.11sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d5328ac98c482785cc16de1dd7be89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Books: 100%|██████████| 1/1 [00:00<00:00,  1.24sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for CDs_and_Vinyl:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/CDs_and_Vinyl/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/CDs_and_Vinyl/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e8bed5f1b74feaafa3470f1c1fc821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->CDs_and_Vinyl: 100%|██████████| 1/1 [00:00<00:00,  1.16sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Cell_Phones_and_Accessories:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Cell_Phones_and_Accessories/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Cell_Phones_and_Accessories/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0670724b15ba49a0a5a5d6532e1345e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Cell_Phones_and_Accessories: 100%|██████████| 1/1 [00:00<00:00,  1.27sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Handmade_Products:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Handmade_Products/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Handmade_Products/prompt_tuning/trained_lora_adapter\n",
      "  samples=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b643c9fc06c141bd8d50c2cf78b36015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Handmade_Products: 100%|██████████| 2/2 [00:01<00:00,  1.02sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Movies_and_TV:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Movies_and_TV/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Movies_and_TV/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d6958733af49d1b4e4e43d4b0227d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Movies_and_TV: 100%|██████████| 3/3 [00:02<00:00,  1.22sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Toys_and_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Toys_and_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Toys_and_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0110e115ee049ea80b4b59bb9cd8cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Toys_and_Games: 100%|██████████| 176/176 [02:28<00:00,  1.18sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Video_Games:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Video_Games/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Video_Games/prompt_tuning/trained_lora_adapter\n",
      "  samples=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07351d288dab4feea2fbc722e9e4e1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->Video_Games: 100%|██████████| 4/4 [00:02<00:00,  1.60sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47db9de21ab742de904d23722cb4c8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Toys_and_Games->amazon_home: 100%|██████████| 3/3 [00:02<00:00,  1.16sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (Toys_and_Games): 0.3177\n",
      "Validity@1 (Toys_and_Games): 1.0000, invalid entries: 0 / 192\n",
      "Test details saved to plora_test_out_v3/Toys_and_Games/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run with CONF_THRESHOLD = 0.5\n",
    "evaluate_category(\"Toys_and_Games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fcf0017-10dc-4616-863a-1a9b555498d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Appliances\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Appliances.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Appliances\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Appliances/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Appliances\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Appliances: 100%|██████████| 230/230 [00:03<00:00, 63.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Appliances. Samples: 230\n",
      "[DEBUG] Load model for Appliances:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Appliances/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Appliances/prompt_tuning/trained_lora_adapter\n",
      "  samples=222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574e0624031940369ba0dd24502dd82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->Appliances: 100%|██████████| 222/222 [02:57<00:00,  1.25sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Pet_Supplies:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Pet_Supplies/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Pet_Supplies/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3705eb22a1c499e935acedc76344b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->Pet_Supplies: 100%|██████████| 1/1 [00:00<00:00,  1.32sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for Software:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Software/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Software/prompt_tuning/trained_lora_adapter\n",
      "  samples=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a8161e7ad94a1cbc537df1b3697d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->Software: 100%|██████████| 1/1 [00:00<00:00,  1.16sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Load model for amazon_home:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/amazon_home/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/amazon_home/prompt_tuning/trained_lora_adapter\n",
      "  samples=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b814372cd4f7494eabaffa5a7eaa6670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Appliances->amazon_home: 100%|██████████| 6/6 [00:05<00:00,  1.14sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Recall@1 (Appliances): 0.3522\n",
      "Validity@1 (Appliances): 0.9957, invalid entries: 1 / 230\n",
      "Test details saved to plora_test_out_v3/Appliances/test_results_candidates_st_router_top3_vl.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run with CONF_THRESHOLD = 0.5\n",
    "evaluate_category(\"Appliances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7e55d-7ff1-4fb0-ad3d-a308f213186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amz are in new_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1ef60-c37a-4951-8679-fc3ac4bb5d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be27b0fe-28bb-49bd-b4bc-901a2a8054a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SWEEP] category=Books CONF_THRESHOLD=0.3\n",
      "\n",
      "========================================\n",
      "[DEBUG] Start category: Books\n",
      "ITEM_META_PATH: /root/autodl-tmp/lavic/data/item2meta_train_Books.with_desc.json\n",
      "IMAGE_DIR: /root/autodl-tmp/lavic/data/train_images/Books\n",
      "TEST_DATA_PATH: /root/autodl-tmp/lavic/data/Books/test.jsonl\n",
      "FINETUNE_OUTPUT_DIR: ./plora_test_out_v3/Books\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Routing Books: 100%|██████████| 472/472 [00:05<00:00, 87.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Routing complete for Books. Samples: 472\n",
      "[DEBUG] Load model for Books:\n",
      "  vision=/root/autodl-tmp/lavic/src/out_distilled/Books/lora_adapter_best\n",
      "  lm=/root/autodl-tmp/lavic/src/out_finetuned/Books/prompt_tuning/trained_lora_adapter\n",
      "  samples=452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96256c865a8e45b4a716354286126ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Evaluating Books->Books:  52%|█████▏    | 235/452 [02:23<02:12,  1.63sample/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:189\u001b[39m, in \u001b[36mBatchFeature.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m[key] = tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:142\u001b[39m, in \u001b[36mBatchFeature._get_is_as_tensor_fns.<locals>.as_tensor\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np.ndarray):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m CONF_THRESHOLD = tau\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[SWEEP] category=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m CONF_THRESHOLD=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONF_THRESHOLD\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mevaluate_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m test_data_path = DATA_ROOT / category / \u001b[33m\"\u001b[39m\u001b[33mtest.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m test_data = load_jsonl(test_data_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 242\u001b[39m, in \u001b[36mevaluate_category\u001b[39m\u001b[34m(category)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(sample_ids, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCATEGORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33msample\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    241\u001b[39m     batch = [test_ds[entry_idx]]\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     inputs = \u001b[43mcollator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     input_ids = inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE)\n\u001b[32m    244\u001b[39m     attention_mask = inputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mDataCollatorForLLaVA.__call__\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    174\u001b[39m     all_images.extend(imgs)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_images:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     images_processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     images_tensor = images_processed[\u001b[33m'\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/image_processing_utils.py:51\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/llava_next/image_processing_llava_next.py:719\u001b[39m, in \u001b[36mLlavaNextImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, image_grid_pinpoints, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, do_convert_rgb, return_tensors, data_format, input_data_format)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_pad:\n\u001b[32m    717\u001b[39m     processed_images = \u001b[38;5;28mself\u001b[39m._pad_for_batching(processed_images)\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchFeature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_sizes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:79\u001b[39m, in \u001b[36mBatchFeature.__init__\u001b[39m\u001b[34m(self, data, tensor_type)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m, tensor_type: Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m, TensorType] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(data)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:195\u001b[39m, in \u001b[36mBatchFeature.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type)\u001b[39m\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_values\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing values of different lengths. \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    196\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate padding \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwith \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         )\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "# Threshold sweep for Books\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if 'threshold_sweeps' not in globals():\n",
    "    threshold_sweeps = {}\n",
    "\n",
    "category = \"Books\"\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "rows = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    CONF_THRESHOLD = tau\n",
    "    print(f\"[SWEEP] category={category} CONF_THRESHOLD={CONF_THRESHOLD:.1f}\")\n",
    "    evaluate_category(category)\n",
    "\n",
    "    test_data_path = DATA_ROOT / category / \"test.jsonl\"\n",
    "    test_data = load_jsonl(test_data_path)\n",
    "    ground_truths = [entry.get('gt_items', []) for entry in test_data]\n",
    "\n",
    "    model_key = 'st' if CANDIDATE_TYPE == 'candidates_st' else 'gpt_large'\n",
    "    recommended_field = 'recommended_st' if model_key == 'st' else f\"recommended_{model_key}\"\n",
    "    out_file = Path(f\"./plora_test_out_v3/{category}\") / f\"test_results_{CANDIDATE_TYPE}_router_top3_vl.jsonl\"\n",
    "\n",
    "    results = load_jsonl(out_file)\n",
    "    recommended_ids = [entry.get(recommended_field) for entry in results]\n",
    "    recall = evaluate_recall_at_k(recommended_ids, ground_truths, k=1)\n",
    "    validity, _, _ = check_validity(out_file, model_key)\n",
    "\n",
    "    rows.append({\n",
    "        'category': category,\n",
    "        'tau': tau,\n",
    "        'recall': recall,\n",
    "        'validity': validity,\n",
    "    })\n",
    "\n",
    "threshold_sweeps[category] = rows\n",
    "print(f\"[SWEEP] done for {category}: {len(rows)} thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78338f-bc0a-41da-806d-2271c06ed2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep for Toys_and_Games\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if 'threshold_sweeps' not in globals():\n",
    "    threshold_sweeps = {}\n",
    "\n",
    "category = \"Toys_and_Games\"\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "rows = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    CONF_THRESHOLD = tau\n",
    "    print(f\"[SWEEP] category={category} CONF_THRESHOLD={CONF_THRESHOLD:.1f}\")\n",
    "    evaluate_category(category)\n",
    "\n",
    "    test_data_path = DATA_ROOT / category / \"test.jsonl\"\n",
    "    test_data = load_jsonl(test_data_path)\n",
    "    ground_truths = [entry.get('gt_items', []) for entry in test_data]\n",
    "\n",
    "    model_key = 'st' if CANDIDATE_TYPE == 'candidates_st' else 'gpt_large'\n",
    "    recommended_field = 'recommended_st' if model_key == 'st' else f\"recommended_{model_key}\"\n",
    "    out_file = Path(f\"./plora_test_out_v3/{category}\") / f\"test_results_{CANDIDATE_TYPE}_router_top3_vl.jsonl\"\n",
    "\n",
    "    results = load_jsonl(out_file)\n",
    "    recommended_ids = [entry.get(recommended_field) for entry in results]\n",
    "    recall = evaluate_recall_at_k(recommended_ids, ground_truths, k=1)\n",
    "    validity, _, _ = check_validity(out_file, model_key)\n",
    "\n",
    "    rows.append({\n",
    "        'category': category,\n",
    "        'tau': tau,\n",
    "        'recall': recall,\n",
    "        'validity': validity,\n",
    "    })\n",
    "\n",
    "threshold_sweeps[category] = rows\n",
    "print(f\"[SWEEP] done for {category}: {len(rows)} thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d47f5d-13ff-449d-9bb8-c43a1188beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep for Appliances\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if 'threshold_sweeps' not in globals():\n",
    "    threshold_sweeps = {}\n",
    "\n",
    "category = \"Appliances\"\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "rows = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    CONF_THRESHOLD = tau\n",
    "    print(f\"[SWEEP] category={category} CONF_THRESHOLD={CONF_THRESHOLD:.1f}\")\n",
    "    evaluate_category(category)\n",
    "\n",
    "    test_data_path = DATA_ROOT / category / \"test.jsonl\"\n",
    "    test_data = load_jsonl(test_data_path)\n",
    "    ground_truths = [entry.get('gt_items', []) for entry in test_data]\n",
    "\n",
    "    model_key = 'st' if CANDIDATE_TYPE == 'candidates_st' else 'gpt_large'\n",
    "    recommended_field = 'recommended_st' if model_key == 'st' else f\"recommended_{model_key}\"\n",
    "    out_file = Path(f\"./plora_test_out_v3/{category}\") / f\"test_results_{CANDIDATE_TYPE}_router_top3_vl.jsonl\"\n",
    "\n",
    "    results = load_jsonl(out_file)\n",
    "    recommended_ids = [entry.get(recommended_field) for entry in results]\n",
    "    recall = evaluate_recall_at_k(recommended_ids, ground_truths, k=1)\n",
    "    validity, _, _ = check_validity(out_file, model_key)\n",
    "\n",
    "    rows.append({\n",
    "        'category': category,\n",
    "        'tau': tau,\n",
    "        'recall': recall,\n",
    "        'validity': validity,\n",
    "    })\n",
    "\n",
    "threshold_sweeps[category] = rows\n",
    "print(f\"[SWEEP] done for {category}: {len(rows)} thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c30306-091d-4a1c-9840-d693584d1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize sweeps: table + line plot (no pandas)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'threshold_sweeps' not in globals() or not threshold_sweeps:\n",
    "    raise RuntimeError(\"threshold_sweeps is empty. Run the per-category sweep cells first.\")\n",
    "\n",
    "rows = []\n",
    "for cat, items in threshold_sweeps.items():\n",
    "    rows.extend(items)\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No sweep results found.\")\n",
    "\n",
    "# Table (sorted)\n",
    "rows_sorted = sorted(rows, key=lambda r: (r['category'], r['tau']))\n",
    "print(\"category\\ttau\\trecall\\tvalidity\")\n",
    "for r in rows_sorted:\n",
    "    print(f\"{r['category']}\\t{r['tau']:.1f}\\t{r['recall']:.4f}\\t{r['validity']:.4f}\")\n",
    "\n",
    "# Best tau per category (by recall)\n",
    "best_by_cat = {}\n",
    "for r in rows_sorted:\n",
    "    key = r['category']\n",
    "    if key not in best_by_cat or r['recall'] > best_by_cat[key]['recall']:\n",
    "        best_by_cat[key] = r\n",
    "print(\"\\nBest tau per category (by recall):\")\n",
    "print(\"category\\ttau\\trecall\\tvalidity\")\n",
    "for key in sorted(best_by_cat.keys()):\n",
    "    r = best_by_cat[key]\n",
    "    print(f\"{r['category']}\\t{r['tau']:.1f}\\t{r['recall']:.4f}\\t{r['validity']:.4f}\")\n",
    "\n",
    "# Line plot: recall vs tau\n",
    "plt.figure(figsize=(7, 4))\n",
    "for cat in sorted({r['category'] for r in rows_sorted}):\n",
    "    sub = [r for r in rows_sorted if r['category'] == cat]\n",
    "    taus = [r['tau'] for r in sub]\n",
    "    recalls = [r['recall'] for r in sub]\n",
    "    plt.plot(taus, recalls, marker='o', label=cat)\n",
    "\n",
    "plt.title('Recall@1 vs CONF_THRESHOLD')\n",
    "plt.xlabel('CONF_THRESHOLD (tau)')\n",
    "plt.ylabel('Recall@1')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17c246-ea63-4d63-a9f8-d195f14cb00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
